{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter1 - nn.Module.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fantajeon/DLPytorch1.2/blob/master/Chapter1_nn_Module.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9YNba7eTeHc",
        "colab_type": "text"
      },
      "source": [
        "#nn.Module로 모델 감싸기#\n",
        "이번 강의는 $\\hat{y} = wx + b$을 직접적으로 계산하였다면, PyTorch에서 권장하는 nn.Module로 만드는 과정을 배워보겠습니다. 보통 작게 할때는 별 문제가 되지 않지만, 큰 모델을 설계할때는 시스템의 힘을 빌려야 합니다. nn.Module을 사용하면, 모델의 파라미터 추출 및 내부 저장, 모듈화, 기존의 잘 알려진 강력한 모듈을 쉽게 가져다 쓸수 있습니다. 즉, 많은 이점이 있습니다. 자주 쓰시다 보면 느끼게 되실 겁니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0IoqQCLYl_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 본 강좌에서 필요한 패키지를 Python에 import 합니다.\n",
        "import torch\n",
        "from torch import nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-pkkOS5VSyx",
        "colab_type": "text"
      },
      "source": [
        "# $wx+b$를 nn.Module화 하기 #\n",
        "nn.Module화를 하려면 다음의 3가지 기능을 최소로 구현해야 합니다.\n",
        "1. nn.Module로 상속하기\n",
        "2. \\__init__()\n",
        "3. forward() 정의\n",
        "\n",
        "\n",
        "**nn.Module**은 PyTorch에서 Module을 대표하는 가장 기초 클래스입니다.\n",
        "\n",
        "**\\__init__()**은 클래스를 초기화 하는 함수입니다. 보통 모델에 필요한 변수들을 정의합니다.\n",
        "\n",
        "**forward()**는 Computation Graph에 기록할 연산 과정을 이 함수에서 실제로 구현합니다. 그리고 모듈 밖에서 보면 프로그래밍에서 의미하는 함수를 호출하는 거와 비슷합니다. 예를들면 다음 코드와 같습니다:\n",
        "``` python\n",
        "# Python 함수 정의\n",
        "def f(x): \n",
        "  return wx + b\n",
        "y = f(x)   # 함수 호출하면 변수 y에 결과값 저장\n",
        "```\n",
        "\n",
        "아래 코드에서 nn.Parameter()가 있습니다. 모델의 인자를 정의하는 함수인데, 이것은 torch.tensor와 같습니다. 그런데 다른 한가지 기능이 더 있습니다. 이 함수는 nn.Module과 긴밀히 상호작용을 합니다. 예를들면, 자동으로 \"나는 이 클래스의 인자!\"야라고 자동으로 인자 리스트에 추가됩니다. 이로 인하여 향후에 PyTorch에서 제공하는 최적화 패키지들과 쉽게 연동할 수 있습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5HTeHcvTY-F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4ec26873-98ef-4d97-efdc-77bf6ee1ee6e"
      },
      "source": [
        "#nn.Module화 하기\n",
        "class Model(nn.Module): # 1. 상속\n",
        "  def __init__(self):   # 2. 모듈 초기화 구현\n",
        "    super(Model, self).__init__()\n",
        "    self.w = nn.Parameter( torch.rand(1, dtype=torch.float) )   # 모델의 w 인자 정의(이런게 있군요 눈으로만 여겨 보자!)\n",
        "    self.b = nn.Parameter( torch.rand(1, dtype=torch.float) )   # 모델의 b 인자 정의\n",
        "\n",
        "  def forward(self, x): # 3. forward()구현\n",
        "    return self.w * x + self.b\n",
        "\n",
        "# 테스트 코드\n",
        "f = Model()   # 이제 Module의 클래스를 인스턴스화 합니다.\n",
        "test_x = torch.rand(3, dtype=torch.float32)\n",
        "print( \"f(x) = \", f(test_x) )\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "f(x) =  tensor([0.2009, 0.2027, 0.2040], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tkaGBPUY_t6",
        "colab_type": "text"
      },
      "source": [
        "## 모델의 파라미터를 확인해 보자 ##\n",
        "nn.Parameter()로 모델에 자동등록 된다고 했습니다. 그러면 어떻게 등록되었는지를 확인해 보겠습니다. 보통은 named_parameter()로 호출합니다.\n",
        "### nn.Parameter() ###\n",
        "이 함수는 2개의 인자를 받습니다:\n",
        "1. data: 하나는 변수에 채월질 초기화 데이터\n",
        "2. requires_grad: backward시에 미분값을 계산한다\n",
        "  * 기본은 True로 소유한다\n",
        "  * False면 소유하지 않는다.\n",
        "\n",
        "### nn.Module.named_parameters() ###\n",
        "모델의 인자 리스트를 변수명과 함께 같이 반환한다. \n",
        "다음과 같이 2개의 인자를 가지고 있습니다:\n",
        "1. prefix: 접두어를 붙인다. 기본은 공백이다.\n",
        "2. recurse: Module내에 서브 Module까지의 인자를 모두 가져온다.\n",
        "  * 기본은 True로 모두 가져온다.\n",
        "백문이 불여 일견입니다. 한번 코드를 실행해겠습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09C5BgcswBiW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "0de50983-b1fc-4d36-8898-5438c1ead48b"
      },
      "source": [
        "mymodel = Model()\n",
        "\n",
        "print(\"####  기본값 사용\")\n",
        "for name, param in mymodel.named_parameters():\n",
        "  print(\"=> var name\", name)\n",
        "  print(\"=> param\", \"data:\", param.data, \"size:\", param.size(), \"dim:\", param.dim())\n",
        "\n",
        "print(\"####   prefix=aa 사용\")\n",
        "for name, param in mymodel.named_parameters(prefix=\"aa\"):\n",
        "  print(\"=> var name\", name)\n",
        "  print(\"=> param\", \"data:\", param.data, \"size:\", param.size(), \"dim:\", param.dim())\n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "####  기본값 사용\n",
            "=> var name w\n",
            "=> param data: tensor([0.5923]) size: torch.Size([1]) dim: 1\n",
            "=> var name b\n",
            "=> param data: tensor([0.9532]) size: torch.Size([1]) dim: 1\n",
            "####   prefix=aa 사용\n",
            "=> var name aa.w\n",
            "=> param data: tensor([0.5923]) size: torch.Size([1]) dim: 1\n",
            "=> var name aa.b\n",
            "=> param data: tensor([0.9532]) size: torch.Size([1]) dim: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SMASFrqxi02",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "41a54352-c308-4df3-8bbc-ea0f558a9397"
      },
      "source": [
        "# 서브 모듈을 강제로 만들어 보겠습니다.\n",
        "class Temp(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Temp, self).__init__()\n",
        "    self.z = nn.Parameter(torch.rand(1), requires_grad=True)\n",
        "    self.f = Model()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.f(x) + self.z\n",
        "\n",
        "print(\"####   recurse=True 사용\")\n",
        "g = Temp()\n",
        "for name, param in g.named_parameters(recurse=True):\n",
        "  print(\"=> var name\", name)\n",
        "  print(\"=> param\", \"data:\", param.data, \"size:\", param.size(), \"dim:\", param.dim())\n",
        "  \n",
        "print(\"####   recurse=False 사용\")\n",
        "for name, param in g.named_parameters(recurse=False):\n",
        "  print(\"=> var name\", name)\n",
        "  print(\"=> param\", \"data:\", param.data, \"size:\", param.size(), \"dim:\", param.dim())"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "####   recurse=True 사용\n",
            "=> var name z\n",
            "=> param data: tensor([0.7470]) size: torch.Size([1]) dim: 1\n",
            "=> var name f.w\n",
            "=> param data: tensor([0.0072]) size: torch.Size([1]) dim: 1\n",
            "=> var name f.b\n",
            "=> param data: tensor([0.3880]) size: torch.Size([1]) dim: 1\n",
            "####   recurse=False 사용\n",
            "=> var name z\n",
            "=> param data: tensor([0.7470]) size: torch.Size([1]) dim: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiL4UzR80FU6",
        "colab_type": "text"
      },
      "source": [
        "#Gradient Descendent Optimization에 적용#\n",
        "직접적으로 $y = wx + b$를 했다면, 이젠 Model 클래스의 인스턴스로 바꿔보겠습니다. 이제는 w, b를 직접 지정하지 않았고, named_parameters와 for 루프의 조합으로 끝을 냈습니다.\n",
        "\n",
        "\\* 참고로 gradient는 각 변수의 어느 쪽으로 변화하는 지를 가지고 있는 백터입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyqx5WP10uDr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "78d89fbe-81ac-41d7-b107-68b8a0e25149"
      },
      "source": [
        "x = torch.tensor([0.1, 0.2, 0.3, 0.4, 0.5, 0.6], dtype=torch.float32)\n",
        "y = torch.tensor([15, 25, 40, 55, 65, 66], dtype=torch.float32)\n",
        "\n",
        "\n",
        "def loss(pred_y, y):\n",
        "  return 0.5*(pred_y - y).pow(2.0).sum()\n",
        "\n",
        "learning_rate = 0.0001\n",
        "\n",
        "model = Model() # 모델 인스턴스화\n",
        "for step in range(300000):\n",
        "  # PyTorch는 Dynamic하게 연산을 기록하는 기능 때문에, \n",
        "  # 매번 loop에서 x--> predict -> loss-->backward()-->gradient descent를 해줘야 함.\n",
        "  pred_y = model(x)\n",
        "  L = loss(pred_y, y)\n",
        "  model.zero_grad() # gradient[편미분을 모아 놓은 벡터] 초기화\n",
        "  L.backward()\n",
        "  # torch.no_grad()는 computation graph에서 backward()시 제외된다. 예측만 할때 유용하다.\n",
        "  with torch.no_grad():\n",
        "    for name, param in model.named_parameters():\n",
        "      param -= learning_rate * param.grad\n",
        "\n",
        "  if step % 10000 == 0:\n",
        "    print(\"Step:{}, L:{:.5}, w={:.3}, b={:.3}, grad(w)={:.3}, grad(b)={:.3}\".format(step, L.item(), model.w.item(), model.b.item(), model.w.grad.item(), model.b.grad.item()))\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step:0, L:6858.2, w=0.751, b=0.355, grad(w)=-1.11e+02, grad(b)=-2.62e+02\n",
            "Step:10000, L:643.36, w=28.9, b=34.9, grad(w)=-12.9, grad(b)=4.3\n",
            "Step:20000, L:483.53, w=40.8, b=30.7, grad(w)=-11.0, grad(b)=3.95\n",
            "Step:30000, L:366.41, w=50.9, b=27.1, grad(w)=-9.4, grad(b)=3.38\n",
            "Step:40000, L:280.59, w=59.6, b=23.9, grad(w)=-8.05, grad(b)=2.89\n",
            "Step:50000, L:217.7, w=67.1, b=21.3, grad(w)=-6.89, grad(b)=2.48\n",
            "Step:60000, L:171.62, w=73.5, b=19.0, grad(w)=-5.9, grad(b)=2.12\n",
            "Step:70000, L:137.85, w=78.9, b=17.0, grad(w)=-5.05, grad(b)=1.81\n",
            "Step:80000, L:113.11, w=83.6, b=15.3, grad(w)=-4.32, grad(b)=1.55\n",
            "Step:90000, L:94.972, w=87.6, b=13.9, grad(w)=-3.7, grad(b)=1.33\n",
            "Step:100000, L:81.684, w=91.1, b=12.7, grad(w)=-3.17, grad(b)=1.14\n",
            "Step:110000, L:71.948, w=94.0, b=11.6, grad(w)=-2.71, grad(b)=0.976\n",
            "Step:120000, L:64.813, w=96.5, b=10.7, grad(w)=-2.32, grad(b)=0.835\n",
            "Step:130000, L:59.588, w=98.6, b=9.93, grad(w)=-1.99, grad(b)=0.712\n",
            "Step:140000, L:55.755, w=1e+02, b=9.27, grad(w)=-1.7, grad(b)=0.611\n",
            "Step:150000, L:52.949, w=1.02e+02, b=8.7, grad(w)=-1.46, grad(b)=0.521\n",
            "Step:160000, L:50.889, w=1.03e+02, b=8.22, grad(w)=-1.25, grad(b)=0.448\n",
            "Step:170000, L:49.384, w=1.05e+02, b=7.8, grad(w)=-1.07, grad(b)=0.381\n",
            "Step:180000, L:48.279, w=1.06e+02, b=7.45, grad(w)=-0.916, grad(b)=0.326\n",
            "Step:190000, L:47.468, w=1.06e+02, b=7.14, grad(w)=-0.785, grad(b)=0.277\n",
            "Step:200000, L:46.874, w=1.07e+02, b=6.88, grad(w)=-0.671, grad(b)=0.241\n",
            "Step:210000, L:46.436, w=1.08e+02, b=6.66, grad(w)=-0.572, grad(b)=0.213\n",
            "Step:220000, L:46.118, w=1.08e+02, b=6.47, grad(w)=-0.493, grad(b)=0.176\n",
            "Step:230000, L:45.883, w=1.09e+02, b=6.31, grad(w)=-0.419, grad(b)=0.157\n",
            "Step:240000, L:45.714, w=1.09e+02, b=6.17, grad(w)=-0.36, grad(b)=0.136\n",
            "Step:250000, L:45.591, w=1.09e+02, b=6.05, grad(w)=-0.313, grad(b)=0.107\n",
            "Step:260000, L:45.494, w=1.1e+02, b=5.95, grad(w)=-0.267, grad(b)=0.0929\n",
            "Step:270000, L:45.43, w=1.1e+02, b=5.86, grad(w)=-0.231, grad(b)=0.0789\n",
            "Step:280000, L:45.376, w=1.1e+02, b=5.78, grad(w)=-0.191, grad(b)=0.0787\n",
            "Step:290000, L:45.343, w=1.1e+02, b=5.73, grad(w)=-0.173, grad(b)=0.0548\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hgf1mwMAuRt",
        "colab_type": "text"
      },
      "source": [
        "# 전체 코드를 하나로 모은 내용 #"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvMWR8dwXint",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "d9ed9aba-b9d9-4876-af37-18b579ad680a"
      },
      "source": [
        "#nn.Module화 하기\n",
        "class Model(nn.Module): # 1. 상속\n",
        "  def __init__(self):   # 2. 모듈 초기화 구현\n",
        "    super(Model, self).__init__()\n",
        "    self.w = nn.Parameter( torch.rand(1, dtype=torch.float) )   # 모델의 w 인자 정의(이런게 있군요 눈으로만 여겨 보자!)\n",
        "    self.b = nn.Parameter( torch.rand(1, dtype=torch.float) )   # 모델의 b 인자 정의\n",
        "\n",
        "  def forward(self, x): # 3. forward()구현\n",
        "    return self.w * x + self.b\n",
        "\n",
        "def loss(pred_y, y):\n",
        "  return 0.5*(pred_y - y).pow(2.0).sum()\n",
        "\n",
        "learning_rate = 0.0001\n",
        "\n",
        "x = torch.tensor([0.1, 0.2, 0.3, 0.4, 0.5, 0.6], dtype=torch.float32)\n",
        "y = torch.tensor([15, 25, 40, 55, 65, 66], dtype=torch.float32)\n",
        "\n",
        "model = Model() # 모델 인스턴스화\n",
        "for step in range(300000):\n",
        "  # PyTorch는 Dynamic하게 연산을 기록하는 기능 때문에, \n",
        "  # 매번 loop에서 x--> predict -> loss-->backward()-->gradient descent를 해줘야 함.\n",
        "  pred_y = model(x)\n",
        "  L = loss(pred_y, y)\n",
        "  model.zero_grad() # gradient[편미분을 모아 놓은 벡터] 초기화\n",
        "  L.backward()\n",
        "  # torch.no_grad()는 computation graph에서 backward()시 제외된다. 예측만 할때 유용하다.\n",
        "  with torch.no_grad():\n",
        "    for name, param in model.named_parameters():\n",
        "      param -= learning_rate * param.grad\n",
        "\n",
        "  if step % 10000 == 0:\n",
        "    print(\"Step:{}, L:{:.5}, w={:.3}, b={:.3}, grad(w)={:.3}, grad(b)={:.3}\".format(step, L.item(), model.w.item(), model.b.item(), model.w.grad.item(), model.b.grad.item()))\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step:0, L:6839.3, w=0.881, b=0.372, grad(w)=-1.11e+02, grad(b)=-2.62e+02\n",
            "Step:10000, L:642.0, w=29.0, b=34.9, grad(w)=-12.9, grad(b)=4.29\n",
            "Step:20000, L:482.53, w=40.8, b=30.7, grad(w)=-11.0, grad(b)=3.94\n",
            "Step:30000, L:365.68, w=51.0, b=27.0, grad(w)=-9.39, grad(b)=3.38\n",
            "Step:40000, L:280.06, w=59.7, b=23.9, grad(w)=-8.04, grad(b)=2.89\n",
            "Step:50000, L:217.31, w=67.1, b=21.2, grad(w)=-6.88, grad(b)=2.47\n",
            "Step:60000, L:171.34, w=73.5, b=19.0, grad(w)=-5.89, grad(b)=2.12\n",
            "Step:70000, L:137.64, w=79.0, b=17.0, grad(w)=-5.04, grad(b)=1.81\n",
            "Step:80000, L:112.95, w=83.7, b=15.3, grad(w)=-4.32, grad(b)=1.55\n",
            "Step:90000, L:94.86, w=87.7, b=13.9, grad(w)=-3.7, grad(b)=1.33\n",
            "Step:100000, L:81.602, w=91.1, b=12.6, grad(w)=-3.16, grad(b)=1.14\n",
            "Step:110000, L:71.887, w=94.0, b=11.6, grad(w)=-2.71, grad(b)=0.976\n",
            "Step:120000, L:64.769, w=96.5, b=10.7, grad(w)=-2.32, grad(b)=0.834\n",
            "Step:130000, L:59.555, w=98.7, b=9.92, grad(w)=-1.99, grad(b)=0.711\n",
            "Step:140000, L:55.731, w=1e+02, b=9.26, grad(w)=-1.7, grad(b)=0.61\n",
            "Step:150000, L:52.931, w=1.02e+02, b=8.7, grad(w)=-1.46, grad(b)=0.52\n",
            "Step:160000, L:50.877, w=1.03e+02, b=8.21, grad(w)=-1.25, grad(b)=0.447\n",
            "Step:170000, L:49.374, w=1.05e+02, b=7.8, grad(w)=-1.07, grad(b)=0.381\n",
            "Step:180000, L:48.272, w=1.06e+02, b=7.45, grad(w)=-0.915, grad(b)=0.326\n",
            "Step:190000, L:47.463, w=1.06e+02, b=7.14, grad(w)=-0.784, grad(b)=0.277\n",
            "Step:200000, L:46.871, w=1.07e+02, b=6.88, grad(w)=-0.67, grad(b)=0.241\n",
            "Step:210000, L:46.433, w=1.08e+02, b=6.66, grad(w)=-0.572, grad(b)=0.212\n",
            "Step:220000, L:46.116, w=1.08e+02, b=6.47, grad(w)=-0.492, grad(b)=0.176\n",
            "Step:230000, L:45.882, w=1.09e+02, b=6.31, grad(w)=-0.419, grad(b)=0.156\n",
            "Step:240000, L:45.713, w=1.09e+02, b=6.17, grad(w)=-0.36, grad(b)=0.136\n",
            "Step:250000, L:45.59, w=1.09e+02, b=6.05, grad(w)=-0.313, grad(b)=0.107\n",
            "Step:260000, L:45.494, w=1.1e+02, b=5.95, grad(w)=-0.266, grad(b)=0.0925\n",
            "Step:270000, L:45.43, w=1.1e+02, b=5.86, grad(w)=-0.231, grad(b)=0.0789\n",
            "Step:280000, L:45.375, w=1.1e+02, b=5.78, grad(w)=-0.191, grad(b)=0.0787\n",
            "Step:290000, L:45.343, w=1.1e+02, b=5.73, grad(w)=-0.172, grad(b)=0.0548\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ay9cUiBYAtuQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}