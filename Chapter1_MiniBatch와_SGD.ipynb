{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter1 - MiniBatch와 SGD.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fantajeon/DLPytorch1.2/blob/master/Chapter1_MiniBatch%EC%99%80_SGD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lxb3cF3UqASk",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#MiniBatch와 SGD#\n",
        "Author: fantajeon@gmail.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLZpVB7jqMHd",
        "colab_type": "text"
      },
      "source": [
        "이번 시간에는 MiniBatch에 대해서 알아보겠습니다. 이것은 우선 Gradient Descent에서부터 시작을 해보겠습니다.\n",
        "\n",
        "## Gradient Descent ##\n",
        "Gradient Descent에서는 전체 훈련셋으로부터 gradient을 계산했습니다. 그런데 여기서 큰 문제가 하나 있습니다. 이 상황을 생각해 봅시다. 메모리에 적재(할당)할 수 없을 정도로 매우 크면 어떻게 해야 하나요? 적제를 할 수 없기 때문에 gradient도 역시 계산을 못할 것입니다. 비록 메모리에 적재까지는 해도, gradient를 계산할때 메모리 한계로 계산 불가이면 어떻게 할까요? 그래서 바로 쉽게 생각할 수 있는 대안은 일부분만으로 해보자! 이 일부분은 크기는 어디까지 일까? 이 문제는 ML 설계자에게 고질적으로 따라다는 문제로 남아있습니다.\n",
        "\n",
        "## Mini Batch와 Stochastic Gradient Descent Optimization ##\n",
        "일 부분을 취하면 Gradient Descent를 하기때문에 별도의 이름이 있습니다. 바로 Stochastic Gradient Descent(SGD)입니다. 자 그럼 루프 구조를 먼저 언급하고 코드로 바로 가겠습니다.\n",
        "1. (epoch loop) 전체 훈련셋을 20번 학습한다.\n",
        "  1. (mini batch loop) 전체 훈련셋으로부터 n개씩 일부분을 차근차근 서브 샘플(MiniBatch라 한다)을 추출한다.\n",
        "    1. Predict: MiniBatch 단위로 $\\hat {y} = f(x)$을 예측한다.\n",
        "    2. Loss: Loss(y, $\\hat {y}$)를 계산한다.\n",
        "    3. Gradient 계산: Backward()를 사용하여 gradient를 계산한다.\n",
        "    4. 인자 갱신, step: Optimizer.step()으로 $f(x)$, 즉 모델의 인자를 갱신한다.\n",
        "\n",
        "### 용어설명 ###\n",
        "**poch**는 전체 훈련셋을 몇 번 학습할 것인지를 지정합니다. 이게 생긴것은 MiniBatch때문에 생긴것입니다. 일부를 학습하지만, 빼먹는 샘플은 없어야 합니다. 그리고 일부분으로 gradient를 계산한 것이기 때문에, 오차가 있기 때문에 최적화하는 데 시간이 더 필요합니다.\n",
        "\n",
        "**MiniBatch**는 훈련셋중 이번에 학습할 샘플입니다. 보통은 학습시킬 훈련셋의 따라서 적당한 양은 직접 찾아야 합니다. 그러나 물리적인 메모리와 학습 속도를 관찰하다보면, 현실적으로 가능한 수치들이 있을 겁니다. 보통은 32, 64, 128, 256, 1024 중에 하나로 합니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_O6sAWiu9al",
        "colab_type": "text"
      },
      "source": [
        "# 강좌를 위해서 큰 훈련셋을 마련 해보자 #"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBim_Q-91NQw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#필요한 모듈 초기화\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "from torch import nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_Ui2Po-6Xod",
        "colab_type": "text"
      },
      "source": [
        "## 실험을 위해서 시뮬레이션 데이터 생성 ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIXr1Joup5k5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d42074f5-7546-44b6-aac7-1d630e2e0527"
      },
      "source": [
        "w = 110.0\n",
        "b = 5.5\n",
        "\n",
        "torch.manual_seed(0)  # 랜덤이긴하지만, 실험 재현 목적으로 0을 임의적으로 설정\n",
        "x = torch.linspace(0,1,steps=102400)\n",
        "y = w*x + b + torch.randn( x.size() ) * 30.0  # 잡음 추가\n",
        "\n",
        "print(\"x의 개수\", x.size())\n",
        "print(y[0:10])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x의 개수 torch.Size([102400])\n",
            "tensor([-28.2752, -29.0697,  -2.0152,  -7.5131,  30.9656,  26.2656,  -3.9739,\n",
            "        -57.9491,  15.1768, -32.3904])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPlF2hxb0nZ_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "5e030e62-0400-4ef3-ed88-3c606733bb04"
      },
      "source": [
        "# 이제 어떤 모양인지 300개 간격으로 그래프를 그려 보겠습니다.\n",
        "\n",
        "# numpy는 공학자들이 많이 쓰는 공학용 계산 framework입니다. PyTorch는 Numpy와 호환이 잘 되도록 설계되었습니다.\n",
        "x_numpy = x[::300].numpy()   \n",
        "y_numpy = y[::300].numpy()\n",
        "# 아래에 detach는 연산을 기록하는 computation graph에서 때낸다. 향후 어떠한 연산을 해도 그래프에 기록되지 않을 것입니다.\n",
        "plt.plot(x_numpy,y_numpy,'ro')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f75691fccf8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZCZQ8AkhZCb8DddYlGRtVot0AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZfSiJP81Wp3",
        "colab_type": "text"
      },
      "source": [
        "# Stochastic Gradient Descent #\n",
        "자 이제 코드를 구현해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdmOWPu7wWBN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fad38ac3-975f-4041-99b4-f0bfdbf80cec"
      },
      "source": [
        "#nn.Module화 하기\n",
        "class Model(nn.Module): # 1. 상속\n",
        "  def __init__(self):   # 2. 모듈 초기화 구현\n",
        "    super(Model, self).__init__()\n",
        "    self.w = nn.Parameter( torch.rand(1, dtype=torch.float) )   # 모델의 w 인자 정의(이런게 있군요 눈으로만 여겨 보자!)\n",
        "    self.b = nn.Parameter( torch.rand(1, dtype=torch.float) )   # 모델의 b 인자 정의\n",
        "\n",
        "  def forward(self, x): # 3. forward()구현\n",
        "    return self.w * x + self.b\n",
        "\n",
        "model = Model()\n",
        "loss = nn.MSELoss(reduction='sum')\n",
        "\n",
        "\n",
        "epoch = 20\n",
        "mini_batch = 1024\n",
        "learning_rate = 1e-4\n",
        "optimizer = torch.optim.SGD(params=model.parameters(), lr=learning_rate)\n",
        "\n",
        "L_list = []\n",
        "\n",
        "nsample = x.size(0)\n",
        "max_step = nsample // mini_batch\n",
        "print(\"훈련셋 개수\", nsample, \", minibatch 반복문 회수\", max_step)\n",
        "L_sum = 0\n",
        "for e_iter in range(epoch):  # epoch loop\n",
        "  for step in range(max_step):   # minibatch loop\n",
        "    mini_batch_x = x[ step*mini_batch:(step+1)*mini_batch]\n",
        "    mini_batch_y = y[ step*mini_batch:(step+1)*mini_batch]\n",
        "    pred_y = model(mini_batch_x)\n",
        "    L = loss(pred_y, mini_batch_y)\n",
        "    L_sum += L\n",
        "    optimizer.zero_grad() \n",
        "    L.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 50 == 0:\n",
        "      print(\"Epoch:{}/Step:{}, L:{:.5}, w={}, b={:.3}, grad(w)={:.3}, grad(b)={:.3}\".format(e_iter, step, L.item(), model.w.item(), model.b.item(), model.w.grad.item(), model.b.grad.item()))\n",
        "    step += 1\n",
        "  print(\"==> End Epoch #{}, loss=L:{:.5}\".format(e_iter, L_sum/max_step))\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "훈련셋 개수 102400 , minibatch 반복문 회수 100\n",
            "Epoch:0/Step:0, L:1.0108e+06, w=0.36468106508255005, b=2.14, grad(w)=-58.8, grad(b)=-1.26e+04\n",
            "Epoch:0/Step:50, L:9.1485e+05, w=12.78161334991455, b=51.6, grad(w)=-3.56e+03, grad(b)=-7.04e+03\n",
            "==> End Epoch #0, loss=L:9.4039e+05\n",
            "Epoch:1/Step:0, L:6.4718e+06, w=33.33009338378906, b=65.1, grad(w)=7.54e+02, grad(b)=1.5e+05\n",
            "Epoch:1/Step:50, L:9.0836e+05, w=39.08966827392578, b=39.1, grad(w)=-2.45e+03, grad(b)=-4.84e+03\n",
            "==> End Epoch #1, loss=L:2.0138e+06\n",
            "Epoch:2/Step:0, L:3.8635e+06, w=54.019134521484375, b=49.0, grad(w)=5.48e+02, grad(b)=1.09e+05\n",
            "Epoch:2/Step:50, L:9.0513e+05, w=58.2474365234375, b=30.0, grad(w)=-1.64e+03, grad(b)=-3.24e+03\n",
            "==> End Epoch #2, loss=L:3.0172e+06\n",
            "Epoch:3/Step:0, L:2.4873e+06, w=69.0851821899414, b=37.3, grad(w)=3.99e+02, grad(b)=7.88e+04\n",
            "Epoch:3/Step:50, L:9.0356e+05, w=72.1983871459961, b=23.4, grad(w)=-1.05e+03, grad(b)=-2.07e+03\n",
            "==> End Epoch #3, loss=L:3.9833e+06\n",
            "Epoch:4/Step:0, L:1.7627e+06, w=80.05648040771484, b=28.7, grad(w)=2.89e+02, grad(b)=5.69e+04\n",
            "Epoch:4/Step:50, L:9.0285e+05, w=82.35763549804688, b=18.6, grad(w)=-6.21e+02, grad(b)=-1.22e+03\n",
            "==> End Epoch #4, loss=L:4.9296e+06\n",
            "Epoch:5/Step:0, L:1.3821e+06, w=88.0459213256836, b=22.5, grad(w)=2.1e+02, grad(b)=4.09e+04\n",
            "Epoch:5/Step:50, L:9.0255e+05, w=89.75576782226562, b=15.0, grad(w)=-3.08e+02, grad(b)=-6.04e+02\n",
            "==> End Epoch #5, loss=L:5.8654e+06\n",
            "Epoch:6/Step:0, L:1.183e+06, w=93.86396026611328, b=17.9, grad(w)=1.52e+02, grad(b)=2.93e+04\n",
            "Epoch:6/Step:50, L:9.0245e+05, w=95.1432113647461, b=12.5, grad(w)=-80.7, grad(b)=-1.54e+02\n",
            "==> End Epoch #6, loss=L:6.7956e+06\n",
            "Epoch:7/Step:0, L:1.0794e+06, w=98.1007308959961, b=14.6, grad(w)=1.1e+02, grad(b)=2.09e+04\n",
            "Epoch:7/Step:50, L:9.0244e+05, w=99.06640625, b=10.6, grad(w)=84.9, grad(b)=1.74e+02\n",
            "==> End Epoch #7, loss=L:7.7228e+06\n",
            "Epoch:8/Step:0, L:1.0259e+06, w=101.18603515625, b=12.2, grad(w)=79.3, grad(b)=1.47e+04\n",
            "Epoch:8/Step:50, L:9.0246e+05, w=101.92334747314453, b=9.27, grad(w)=2.05e+02, grad(b)=4.13e+02\n",
            "==> End Epoch #8, loss=L:8.6484e+06\n",
            "Epoch:9/Step:0, L:9.9853e+05, w=103.43276977539062, b=10.5, grad(w)=56.9, grad(b)=1.03e+04\n",
            "Epoch:9/Step:50, L:9.025e+05, w=104.00379180908203, b=8.28, grad(w)=2.93e+02, grad(b)=5.87e+02\n",
            "==> End Epoch #9, loss=L:9.5731e+06\n",
            "Epoch:10/Step:0, L:9.848e+05, w=105.0688705444336, b=9.2, grad(w)=40.7, grad(b)=6.99e+03\n",
            "Epoch:10/Step:50, L:9.0253e+05, w=105.51880645751953, b=7.57, grad(w)=3.57e+02, grad(b)=7.13e+02\n",
            "==> End Epoch #10, loss=L:1.0497e+07\n",
            "Epoch:11/Step:0, L:9.7807e+05, w=106.26031494140625, b=8.28, grad(w)=28.8, grad(b)=4.62e+03\n",
            "Epoch:11/Step:50, L:9.0256e+05, w=106.62208557128906, b=7.04, grad(w)=4.04e+02, grad(b)=8.06e+02\n",
            "==> End Epoch #11, loss=L:1.1421e+07\n",
            "Epoch:12/Step:0, L:9.7491e+05, w=107.12794494628906, b=7.6, grad(w)=20.2, grad(b)=2.89e+03\n",
            "Epoch:12/Step:50, L:9.0259e+05, w=107.42546844482422, b=6.66, grad(w)=4.38e+02, grad(b)=8.73e+02\n",
            "==> End Epoch #12, loss=L:1.2345e+07\n",
            "Epoch:13/Step:0, L:9.7352e+05, w=107.7597427368164, b=7.11, grad(w)=13.9, grad(b)=1.63e+03\n",
            "Epoch:13/Step:50, L:9.0261e+05, w=108.010498046875, b=6.38, grad(w)=4.62e+02, grad(b)=9.22e+02\n",
            "==> End Epoch #13, loss=L:1.3269e+07\n",
            "Epoch:14/Step:0, L:9.73e+05, w=108.21981048583984, b=6.75, grad(w)=9.31, grad(b)=7.08e+02\n",
            "Epoch:14/Step:50, L:9.0262e+05, w=108.4365234375, b=6.18, grad(w)=4.8e+02, grad(b)=9.57e+02\n",
            "==> End Epoch #14, loss=L:1.4193e+07\n",
            "Epoch:15/Step:0, L:9.7288e+05, w=108.55484771728516, b=6.49, grad(w)=5.98, grad(b)=39.4\n",
            "Epoch:15/Step:50, L:9.0263e+05, w=108.74678039550781, b=6.03, grad(w)=4.94e+02, grad(b)=9.83e+02\n",
            "==> End Epoch #15, loss=L:1.5117e+07\n",
            "Epoch:16/Step:0, L:9.7293e+05, w=108.7988510131836, b=6.3, grad(w)=3.55, grad(b)=-4.47e+02\n",
            "Epoch:16/Step:50, L:9.0264e+05, w=108.97271728515625, b=5.93, grad(w)=5.03e+02, grad(b)=1e+03\n",
            "==> End Epoch #16, loss=L:1.604e+07\n",
            "Epoch:17/Step:0, L:9.7304e+05, w=108.97654724121094, b=6.16, grad(w)=1.79, grad(b)=-8.02e+02\n",
            "Epoch:17/Step:50, L:9.0265e+05, w=109.13724517822266, b=5.85, grad(w)=5.1e+02, grad(b)=1.02e+03\n",
            "==> End Epoch #17, loss=L:1.6964e+07\n",
            "Epoch:18/Step:0, L:9.7316e+05, w=109.10591888427734, b=6.06, grad(w)=0.5, grad(b)=-1.06e+03\n",
            "Epoch:18/Step:50, L:9.0265e+05, w=109.2570571899414, b=5.79, grad(w)=5.15e+02, grad(b)=1.03e+03\n",
            "==> End Epoch #18, loss=L:1.7888e+07\n",
            "Epoch:19/Step:0, L:9.7327e+05, w=109.20014953613281, b=5.98, grad(w)=-0.437, grad(b)=-1.25e+03\n",
            "Epoch:19/Step:50, L:9.0266e+05, w=109.34431457519531, b=5.75, grad(w)=5.19e+02, grad(b)=1.03e+03\n",
            "==> End Epoch #19, loss=L:1.8811e+07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlQoGvLe5uyo",
        "colab_type": "text"
      },
      "source": [
        "## 결과 분석 ##\n",
        "실제 우리가 예상한 $w$와 $b$값을 110과 5.5였습니다. 잡음을 섞어서 정확히 추정을 어려울 것입니다. 그러나 근사한 값은 얻었습니다. $w$와 $b$의 SGD로 추정한 값은 각각 109와 6 이기 때문입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bIfCcjzx6lX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "03781e01-708d-4a40-fd6a-ab8a4837ce42"
      },
      "source": [
        "# 이제 어떤 모양인지 512개 간격으로 그래프를 그려 보겠습니다.\n",
        "\n",
        "# numpy는 공학자들이 많이 쓰는 공학용 계산 framework입니다. PyTorch는 Numpy와 호환이 잘 되도록 설계되었습니다.\n",
        "x_numpy = x.numpy()[::512]   \n",
        "y_numpy = y.numpy()[::512]\n",
        "pred_y = model(x).detach().numpy()[::512]\n",
        "\n",
        "# 아래에 detach는 연산을 기록하는 computation graph에서 때낸다. 향후 어떠한 연산을 해도 그래프에 기록되지 않을 것입니다.\n",
        "plt.plot(x_numpy,y_numpy,'ro')\n",
        "plt.plot(x_numpy,pred_y,label='Fitted line')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f75659390f0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJztnXl8VOX1/z8nAdS4VQOCVTMBRNTQ\n1iVft2+r4gKUtvrtt9raRqutSmu/+qtdAWOrteK+1N2C2iKJu6XSCIZdRFEJokJANiUBJBD2JUCW\nOb8/7gwZhrlznzv3udvc83695pWZyZ17n+cun+fc85x7DjEzBEEQhPynwO8GCIIgCN4ggi8IghAR\nRPAFQRAiggi+IAhCRBDBFwRBiAgi+IIgCBFBBF8QBCEiiOALgiBEBBF8QRCEiNDF7wak0r17dy4t\nLfW7GYIgCKFi3rx5G5i5h9VygRL80tJS1NXV+d0MQRCEUEFEDSrLiUtHEAQhIojgC4IgRAQRfEEQ\nhIgggi8IghARRPAFQRAiggi+IAiCFdXVQGkpUFBg/K2u9rtFORGosExBEITAUV0NDBsGtLQYnxsa\njM8AUFHhX7tyQCx8QRCEbFRWdop9kpYW4/uQIYIvCIKQjcZGe98HGBF8QRCEbJSU2Ps+wIjgC4Ig\nZGPUKKCoaN/vioqM70OGCL4gCEI2KiqA0aOBWAwgMv6OHh26CVtAonQEQRCsqagIpcCnIxa+IAhC\nRBDBFwRBiAgi+IIgCBFBBF8QBCEiiOALgiBEBBF8QRCEiCCCLwiCYEaeZMlMInH4giAImcijLJlJ\nlC18InqOiNYT0cKU724nojVE9HHiNTTlfyOJaDkRLSGiwbobLgiC4Cp5lCUziR2Xzj8BDMnw/cPM\nfEriNREAiOhkAFcAKEv85kkiKnTaWEEQBM/IoyyZSZQFn5lnAdikuPilAF5i5j3M/AWA5QDOyKF9\ngiCElbD7v/MoS2YSHZO2NxLRpwmXzxGJ744BsCplmdWJ7/aDiIYRUR0R1TU3N2tojiAIvpP0fzc0\nAMyd/u8wiH5yoGpoMJKlpRLSLJlJnAr+UwD6AjgFwFoAD9pdATOPZuZyZi7v0aOHw+YIghAI/PZ/\n53p3kTpQAcZglRT9EGfJTOIoSoeZ1yXfE9EYADWJj2sAHJey6LGJ7wRBiAJ++r+dRNdkGqiYDbFf\nuVJ7U73GkYVPREenfPw+gGQEzwQAVxDRAUTUG0A/AB862ZYgCB6hw/fup//byd1FHk7UpmInLPNF\nAHMA9Cei1UR0LYD7iGgBEX0KYCCA3wAAM9cDeAXAIgBvAfg/Zu7Q3npBEPSiy/euq0pULoOPE9HO\nw4nafWDmwLxOP/10FgTBR2IxZkPq933FYvbXVVVl/I7I+FtVZf/3RUX7tqOoyHo9TvqQ6zZ9BkAd\nK2gsGcsGg/Lycq6rq/O7GYIQXQoKDJlLhwiIx71tSzJSJh0rf3q6Dx8w7i5UJ1yrqw33T2OjYdmP\nGhX4iVoimsfM5VbLSS4dQRA6CZJLI1fXjNMatBUVxoASjxt/Ay72dhDBFwShE12+dx04GXzyWLSd\nIIIvCEInqtaxF0/RBmnwyRNE8AVBMEiK+FVXGZ/HjctsHXv1FK1T14ywHzJpKwiCvYnOXCdTBdeQ\nSVtBENSx87BS0B5OCnuSNg8RwRcEwZ6IBymSJ8xJ2nxABF8QBHsiHqTJVL+TtIUMEXxBEKxFPNVt\nUlkJXH11MCZTg+ZeCjgi+IIgZI+IyeQ2GTvWGAz8jnM3uzMpKBC3TgZE8AVBMDB7WCnIbpNMdyYA\n0NGh35efB5PDIviCIGTHbbeJEyFN3pkUZiiZbTUo2dlurpPDZtvwa/BQybDm1UuyZQqCRjJlq8wl\ng6XODJqZ2qgjOyVR5jYS6dluLvvAbBs33KA9IycUs2X6LvKpLxF8QdBEJrHp2pW5Wzf7QuNmymBd\ng4nd9dhd3u6Akm0bhYXaB1BVwReXjiDoJCh+3kx+97Y2oLV13+9UfPFupjjQ5S6yGypqd7u5PHtg\ntq4Ok1pQXkQWqYwKXr3EwhdCTZCKZ5hZpHatVLfJZgXbLZxix11l18LP5dgG0ML3XeRTXyL4Qqhx\n09etqy1BaV+STEKa/nJj0MxFwO3Of4gPXwRfyGNy8fO6hU4fvhdtTQqpC9av0nZzKcHoZBuaty2C\nLwheEyQLn1lflI7TbdrB6aDphYgHEBF8QfCaIPnw/UBH/90uQO73gODS9kXwBcEP/BYUP9Fxh+Nk\n0LDavt8DsovbF8EXBMFbdM1h5DpoWm1fl8st1/a56PJTFXyJwxdyIyjx5kJw0JUnP9cC5Fbb1xHz\n7yT/fgAye4rgC/aRohPuEPZB1O88+Vbb1zEgOUkkZ7F9w1B3GZXbAK9e4tIJCUGLRskH/PYv68Kt\nOQzV9WZbzizmv7hYn9vIqg9p2192TD9+7KFX+buPvsPPvvO5WhsyAPHhC64RpHhzVYI0mZqpLTKI\nmqNzMKyqMgQ+fT/rmhi2oGNcFc8/9Vy+57yreeANz3BseA3HhtfwpY/P5jc+XmO/PwlE8AX3CJs4\nBcl6NmtLpv0Z1EHU68FT9/nmduhnGq3tHTx7WTP/6d8L+MxRUzk2vIb7jnyTK/46np+/8Epee2ix\n4/0ogi+4R5AEVIUgDVAe5ldxBT+Ofba8QAF9uKtlTzu/tXAt/+bl+fz122s5NryG+986kYc9P5df\nn7eKt4yt1rofRfAFdwmSi8QKP1xQZvsnm3iFYRD1Y/C0ygvk5cNdWdiys5Vfn7eKhz0/l/vfOpFj\nw2v467fX8m9ens9vLVzLLXvaXWuDCL4gJPFapLJZwdnaEoZBVHXw1NkXlQRrXj3clUbT1l38/Htf\ncMWY97nvyDc5NryGzxg1hW8dv4BnL2vm1vaOzD/UbISI4AtCEq/dEFaiHgZL3gyVwdONPmYbLHMR\nSgcD0or12/nJGcv50sdn7510HXj/DL574mL+qGETd3TErVciFr4IvuAiXlrPVtZbqnglffdBtejT\nURFzt+6oqqp8meuIx+O8YPUWfqD2M77owZl7Rf67j77Dj01bykubtnE8nkXkzZLYiQ9fBD8ShMF1\n4QS/rGCvsDp+bsyZZHPruLDf2jviPGfFBr59wkI+5+5pHBtew71H1PCP/v4ePzf7c169uSX3difb\nq/E6UBV8MpYNBuXl5VxXV+d3MwQ3ST6lm/q0YlGRvpJ5QUClj6WlxhPK6cRiRjqBMONG38zWWVgI\njB2r5dzZ3daBd5dvQG19E6YuXo9NO1vRrUsBzu3XHYPKeuGik3riyIO76Wm35uNMRPOYudxyOVXB\nJ6LnAHwXwHpmHpD47kgALwMoBbASwA+ZeTMREYBHAAwF0ALgGmb+yGobIvgRIJ+FLpXqauNx+8ZG\n49H5UaP2FaWCAsPeS4fIyCETZtwY1F3aX9t2t2HGZ+sxuX4dZi5Zj52tHTj0gC644KSjMLisF847\noQcOPqBLzuv36jirCr6dnvwTwOMAnk/5bgSAacx8DxGNSHweDuDbAPolXmcCeCrxV4g6AUgg5QkV\nFdnFraQk88BnN9GYl1gNYkmS36ksq4rG/dW8fQ+mLFqH2vomvLdiA9o6GN0POQCXnHIMBpf1xDl9\nu6NbF4dpxpL7ysyg9us4q/h9ki8YlvzClM9LABydeH80gCWJ938H8ONMy2V7iQ8/AgTpISg/CYMP\nP9XHXFzsb3lEh/urYcNOHv32Cv7Bk+9y6Qhj0vVb907nO2vqee4XG7k9GVmjw69uFUbqwn6DG5O2\nGQR/S8p7Sn4GUAPgmyn/mwag3Gr9IvgRIAxC5xVBnrxWiX33eqC2sb/i8TjXr9nKD01ewoMffntv\nZM2Qv83ih6cs4UVfbt0/skbXuZktfNSl46wq+LYmbYmoFEANd/rwtzDzV1L+v5mZjyCiGgD3MPPs\nxPfTAAxn5v0c9EQ0DMAwACgpKTm9IdNtm5BfqLoGBP8wm2tJJ0BzDh1xxkeNm1G7sAmTF61D46YW\nEAHlsSMwuKwXBp3cCyXFReYr0DW/5MP8jKoP32k+/HVEdHRig0cDWJ/4fg2A41KWOzbx3X4w82hm\nLmfm8h49ejhsjhAKci1wkQ+4kfPeyTrNfqs6p+LznENrexwzl6zHyH8twJl3TcPlT8/B2Dkr0afH\nwbj7f7+GD2+5CK/+8hxc960+2cUeMO9zQ4O9/aqrEIwbqNwGJF/Y36VzP4ARifcjANyXeP8dAJNg\nuHnOAvChyvrFpSMEBjfcLW49gZrrOnNJAREAV9yO3W1c88mXfNMLH/GAP7/FseE1fPKfJvGvqufx\nGx+v4W27WnNbsa6cPT64LaHbhw/gRQBrAbQBWA3gWgDFMPzzywBMBXAkd/rznwCwAsACKPjvWQRf\n8AIVIddRKCMTbkxYO1mn3RQQXbsa+8CHOYeNO/bwyx828s//8SH3qzQSk516x2T+46uf8LTFTbyr\ntd16JVbozNnj8fyMdsH34hUIwQ/yRJrgDFXLK5ul58RSc+MJVCfrVE0B4dO1sHpzCz/7zuf8w6ff\n496JyJpz7p7Gt09YyHOefpHbSnu7V1nL7PgHsT4Bi+DnhkSQ5Deq1nC2FMZOLPIwWfg+EI/HeWnT\nNn506lL+zqOz9kbWXPzQTH6g9jNesHqLEVnjxXVqZ98EwEgUwc+FgF0AgmayWbSpF61Zgi6nVp6q\nUNkRELd8+B7R0RHnjxo28d0TF/PA+2fsFfn/eWI2PzVzOX/evGP/H3lxndo5VgEwEkXwcyGMtVoF\ndcyEorhYLeZch7BYiXkuAuLEwvTBOm1t7+B3ljbzreMX8Bmjpuwt+XflM+/z83NWctPWXdlX4NV1\nqrJvAmIkqgq+JE9LJSp5XqKKWY6Xgw4CNm7cf3ki4/JNxe1Eb3bPwZA807CrtQNvL23G5PomTPts\nPbbuasNBXQtx3gk9MHhAT1zQvycOL+qqtrIgXacByYmkGofvu1Wf+vLdwg/I7VlOqEafRH1COtM+\nUHX1eLHP7NRvDfj5unnnHn6tbhVfP7az5N83/lLLv335Y65NL/mXCbN9H6R+h8zC913kU1++Cz5z\nOEVR5QII0kUSNLK5eoLSltRBKCkoxcXui43N62Htll089r0v+Cdj5nCfRMm/M0dN5T/9ewG/m63k\nX6btZjtfg3KdBuS6EsGPEipWRkAskUBSVWXEmKfvm27dvBcS1Rw2VoOCW23JIGbL12/nJ2Ys40tS\nS/49MIPvmbSY5zduViv5l06YztcADD6qgi8+/HxAxY8YEF9jYOnePbMf3w+/cNIvn2teKV1tNvGV\ncyyGhbM/QW19E96qb8Ly9TsAAF8/9nAMLuuFwWU9cfxRhzrbto7zNSTzGzrQXgDFC0Twc0RlEitI\nE1128eLCDeKAqJrALBWdk8op+6SdCjD32DLUnnA2pvQ7C2sOPwqFBYQzSo/E4LKeGFTWC1/9ykHO\nt5nE6fmaaYK+a1fgsMOATZvybgCQSdsokc8+fK/aHUQXgl33TmGh1v2yq3dfntL3DP79t3/Np9xU\nzbHhNXzCb1/nayvu4lfmNvKmHXu0bWs/nB73AOcCcgOIDz9ieBGl44ev0ouJSebgDoipj/pbPQGc\n6rvP8Vht3dXK/56/mm+oquOTRv6HY8NreMCvX+L/993f85v9/5t3HH6Ed/vEyflmta+CMKBrRARf\nUCfXhGJuC2JVlZq46dxeECI/zLDK85IUL5vHat22XVz1/kq+6tkP+PhbjMia8jun8Mh/fcozn3qJ\n9/TuE9x9YoaKhe/WeeQDqoIvPvyoo1pwOlefqhP/ezYfdhjmHtzC7JhdfTUwcaLSPmvYuBO19U2o\nrV+Hjxo3gxmIFRftnXQ99bgjUFBA7vfFLTLto0zkyXnkRhFzIR+prNz/omhpMb5PFeZcio+nX3QN\nDcZnQE30s6171Cjr3+crmYqEDx0KjB1rKnAMYNHuQtROWYrJ9U34rGk7AODkow/DzReegMEDeqJ/\nz0NBFGKRTyW5j6680nyZoqLonUcqtwFevcSl4wOqeUlymdR0OhEapAei3MapOynDvmqnAv7wmJP5\nrwOv5W/+4hmODa/h0hE1fPlT7/GYWSu4ceNOfdu3wi93mdk5pHmC228gPnxBCVVRzsWH7zTJVVAn\nUs3IVdR09DOxr3cXduHpfU7nEYNv5NNvHMex4TXc73fj+Zof3cEvPvoKN2/f7c72dfVP98CgGsEW\n5LkbBUTwBTXcvBh1hDoG6WLM1hYnoulwP23f3cb/+eb3+cbv/YHLbn7FKPl38yv8f5f8kSec+C3e\n1u2g7O1wsn2dGSXdGnjcOm5W6/YQEXxBHbdO2rBZ6Nmw6osT0czhTmjD9t380ocN/LOUkn+n3VjF\nfxxyE0/vU867C7uotyHXOzHV4+um29ApTge7gJzfIviCd1hZUAGwgBxjJQxO3FeKorNq005+5p3P\n+fK0kn9/mVDP76/YwO033LB/O1QEKFfRU/2d6nJ+1KPw4LhZouEaEcEXvMFP/6yXZHuQJ1uVLAeW\nYnxcFS9p2saPTF3KQx/pLPk36KG3+cHalJJ/6euyu49ztVRVxdJs/TfcsG9bvXrILhWP78z2Q9Nd\nggh+NsIsPEHDb/+sV6g+yJNr/xLnZAcV8LzTzuO77n+Nz08p+ff9J2bz0zOX8xeZSv7pIJdrwo5Y\nqjwx3LWrkaHUy3PEx7kXbetgEXxzwi48QSPI/lmdqOa1KSy0bUi0tnfwrKXruXL8p/xfd+5b8m/c\nnJW87h/VwTRQ7F5LVVWZ01CnvoqLve9rAKKrHN0lsAi+OWEXHh3ovMMJsn9WN6n7zUywFPuzc08b\nT1rwJd/80nz+2m1vcWx4DZ946yT+5bg6Hv/Rat7S0tq5zSAbKHbOJZW7pDCdD8yuPD8hFr5O8kF4\nnKBbQFTXl+0hqiBar9moqjI/j7I80LN55x5+tW4VX5dW8u93r3zMk+ubeFdrhpJ/bhooXrs2VRKa\n+TDh6Sviw3eZoFv4bp/A2aysXLen0uZMJ7YfPlsdWFmqKX34cksL//PdL/jHd4znPn94g2PDa/is\nm8bxn+97nd9d1sxtViX/3DJQvLxzSPXfK+63wPXBTSRKx0WCfJJ40TYrK8vNB13Sv/cjKkMHFvtw\n2ZHH8uNDrudLHntn76TrBdc/zfee+1P+uFc/jtvZz24ZKF4ZPqpzH8XFvrlD8gER/GwE9TbQixNY\nxdJysj07g1ZY3Wtp+zAO8Ce9juf7vnUVX3jtk3tF/pLH3uHHpy/jZQPOyH0/6zIC0s95s2Ove99b\nnW+5CH2SsJ4/LqAq+JIeOUiYldkDjDSuOkqyqaSNdVLWz04a5bCWXayuRvsvfokPi3tjcr+zMfmE\ns/DlYUehMN6BMxsXYvCyObh41yp8ddHHxvJOyyc6LfGY6ZgTZW6T7n3vZunIsJ4/LiAlDsOIDd+w\nI6x8qk4sfDtWV5DdaxnY1drOk+ub+HevfMyn3DJhb8m/6/73Vn51wAW86cBDM/chlzs3LyKpcnkq\n125b3Z50DtH54yYQl04IUfF36nTvuHHB2L3Ag+peS7ClpZXHf7Safzmujk/60ySj5N9tb/GvX/yI\nJ376Je/c02bdh1zi1XUel2xzDk73vVVb3RblgJ8/XiGCH1asrG/d/kndF0weWF3rtu7icXNW8pXP\nvL+35N9/3TmFb/nXp/z2kvW8p80isiadqqp9J6it/Na6rWI3rWyVdatGcYlw54wIftgJcwSC04vX\nh4v/i+Yd/PTM5fz9J2ZzaSIx2Xn3Tee73lzEdSs3cUdH3HolmchlANQ9GZlrUjUVApRPJjD4cP6K\n4FsRdIsi3y6CTGQ6Bnb67eAYxuNxXrhmCz84eQkPfvjtvZE1Qx+ZxY9MXcqfrd22f2KyXMhl4NY5\n2Gfan0TGIKCDAOWTCQQ+Xbci+NkIi5gGfVBygtkxUI3Nz+EYtnfE+YPPN/Id/6nn/75nGseG13Dv\nP0zgy39yDz9z8TXc+NwL+vuZiwWs8/x0W0x1tDWfwit9GrxE8LORTxZFWFF5HiDbxa94DHe3tfP0\nxet4+Guf8Ol/nWyU/LtlIv/szvH8Uvl3uLnocHcH/VzPNV2DvRdi6rSt+XQ9+jR4qQq+ljh8IloJ\nYDuADgDtzFxOREcCeBlAKYCVAH7IzJuzrcezOHw3Y4MFNbI9c5CJ9NjqLL/fMbYKM74xELX1TZi5\npBk79rTjkAO64Pz+PTBkQC+c3/8oHNL/eG9iuDPFwBcVAaNHO3+mQoUwxKr7vY904tP+9jQOH4ag\nd0/77j4AIxLvRwC412o9YuFHiGzJ1HJIxtZcdDi/+PVBfM1lt3G/3483Sv7dMZmHv/YJT1+8jne3\npSUmy2aJpVus6YU6QjAJvc+2xX3pHVHw4ZsI/hIARyfeHw1gidV6xIcfIbIdA8UwvsaeMR5Tfilf\n/pN7uHciMdl//+IZvuOC6/iD8gu4PVtkjZ0BJ/0VtnMlX8Q0LOR7lA6ALwB8BGAegGGJ77ak/J9S\nP5u9JEonQHixf2xuIx6P82drjZJ/3/5bZ8m/wT97jB/85k944VG9jcRkKpOimSaHs00ay92gEGBU\nBV+XD/8YZl5DREcBmALgJgATmPkrKctsZuYjMvx2GIBhAFBSUnJ6Qyb/l+AtAfKpxuOM+au2YHJ9\nE2rrm7ByYwuIgNNKjsDgsp4Y9KsrUFqfYd7HzGdqlkuouBh45BHgqqvU5hZkvkcIEKo+fO3J04jo\ndgA7AFwP4HxmXktERwOYycz9s/028snTgoLPE32t7XG8//lG1NY3YcqidVi/fQ+6FhLO7tsdg8t6\n4uKTe+KoQw80Fs42OAH7Jx2rrMzeN7O+my0fRpwmYxMCh6rgd9GwoYMBFDDz9sT7QQDuADABwNUA\n7kn8fcPptgSPaGy0970GWlrb8faSZtTWN2HaZ+uxfXc7DupauE9kzeEHdd3/h0mhShcwYN+BoKEh\ne5bQZN9GjbLOJlpU1LmNsJE+QCb3CyCiHwVU/D7ZXgD6APgk8aoHUJn4vhjANADLAEwFcKTVuiS1\ngovY8Zd7FMW0accefmVuI1/7z7l8QqViyT9VzPpQWGjdN91ROkFCItQyE/I5PciDVwEhCCeS3agk\nF3OvrNncwv+Y/Tlf8fc53GekkZjs7Lum8m1vLOR3lyuU/FMlW4bIKEdo5dNTrbrIg6g9EfwgEJQT\nyY5V50LulWXrtvHjD73K37vuib2RNRf++Q2+763F/MmqzbnlrLEaSLP1OQiDsF+Ihb8/ebBPRPCD\nQFBOJDtWnYY2x+Nx/rhxM987aTFf8MCMzpJ/Vz3IT5x5GS8/8hhnA5/KQBqUwTZoyH7Zn1zzHdk1\nGlw0NETwg0AQbp+rqtT81g7b3Nbewe8ua+Y//3sBn3XXVI4Nr+E+I9/kn4yZw2MvvIq/PDRDfLvb\n+d2jbMlnQ/bLvtg1cnIZNF0eaEXwc0H3heBFpsJs7c10klmdbDbavKu1nWsXruXfvvwxf+MvtUbJ\nv8qJfP3Yufxa3SrevHOPsaDugS8IA6mQP9gVY79TXmdABN8uTkZgM+F1c1RXWXe2SBWzNlisd0tL\nK//ro1X8i+fr+MRbjZJ/X7vtLb75pfk8aUGi5F86uk/2oLjKhPzBjrGXi8HhspEigm8XJ2lsrWp6\nunH7rNLeXE+ytDav+0c1Pz/HKPnXd2Rnyb/K8Z/yrKXrudUqskb3wOc0D48gOEEs/DwQ/FzFMZcD\nqUOUVNrr4CTLVPLv/Ptn8F0TF/G8hhxK/qm4n+zsk0zLy4Sk4AXiw88Dwc9VHO0MFNmSdrlRNMLG\nSRaPx3nB6i38YO1nPOihzpJ/33l0Fj86dSkvadJU8i8Tui4GvwZfP9YdRKLUX4nSCbng5yo6diJG\nsqXdtXtrp9reLCdZe0ec31+xgf8yoZ7Pubuz5N8Pf3w3P3vx1bzquWrrNug4gXXd7tq9S/N7jiWf\niFp/A4YIfi6oCljqcsXFzN26WZ/oZqKm6jpy0t4UdrW287TFTfzHVz/h0+5IlPyrnMg/v3M8v3z6\nd3jDQYepXbBmF3guaQh0TWjZHTjc9KtGbWI5av0NGKqCrz1bphNCkS0zU3bGrl2Bww4DNm0yzz5o\nVdLPxeyL23e3YUYiMdnMz9ZjZ2sHDjmgCwaeeBSGlPXCef172C/5Z5ZVkmjffqqkVTZbV2GhkYJY\nNaOj3bTObpa6jFoZzaj1N2B4li0zclRW7p9Jsa0NOOQQYMMG89+VlJin3XUh++KGHXswZdE61NY3\n4b3lG9HaEUf3Q7rhklO+ikFlvXBO32Ic0KWw8wd2M2SafZ9+0be0GPssm1ibZajs6DD+qmZ0NMuc\nafYbs2NSUmK+DVXcXHcQiVp/w4rKbYBXL+0uHTcmSZyEOmby4RcXa/NzNm7cyWNmreDLnnp3b2TN\nN++dxn/9Tz1/+MXGfUv+pe8bs0pPdt0hubpmUttj9mRwcbHe4yk+fH1Erb+Z8HHSGpH34bt1Ajrx\nVdo5IRSWjcfjvHjtVv7blLSSfw+/zQ9NXsL1Y17guOoDYV27qs1FJNuVFPJ0YXdz8jX9peN4SpSO\nPqLW31R8HvBE8N2aRPLiwGbZRkdHnOtWbuRRby7ic++bzrHhNVw6ooZ/8OS7PPrtFbxyww7rdprt\nGysLOtM6k+IcixkTtm6GV2Y7nlEWG8F/fJ60FsF381Fmt8Ul7eTZU9CFZ/Y+jUf+YDiX3zmFY8Nr\n+Phb3uSfPvsBV7/fwOu27bJcxz4noJsPmenYN1YhrOlt9utpZ0FI4nN+J1XBz98oHZ/rsjqioAA7\nuxyAt/ucjtp+Z2N633JsP/AQFLXuwvmn98bgsl4YeOJROOzADCX/UtaBTMeWyHyCzWrfeBmJkV53\ndccOYOPG/ZeLxYy/Zv3JNCHsU0F2IY/xWW9Uo3R8t+pTX7748FWsP48sxI079vDLcxv52ivv4hN+\n+zrHhtfwKTdV8++//Wue0vcM3tXn+Mw/zNS+bNa42w+ZuUG2NmezriQ+XPAC8eH7LPjMavlbfC6k\nsWZzCz83+3P+0d/f6yz5d+sEvm3wr/i9477GbVSQfZvZHoDS7eawuy90D5Rm63PDfWW3DYIgUTo+\nC74VKtafCxbisnXb+PHpy/itsJ3BAAAN8ElEQVR7j72zN7Lmogdn8v1vfcafrtpi5KxRPXmsLHnd\nJ6Cdp5G9snhymaDO5fhJ6KEQUETwVVCx/jRYiB0dcZ7fuJnvmbSYB6aU/Lv08dn85IzlvGL99uwr\nyCaybk8W5TpoeO1KMWunTpEW95AQUFQFP38nbVVQmWjJcTKmrSOOD7/YhNr6JkyuX4embbvRpYBw\nVp9iDC7riYtP7oVehx9o3UardAFuThbZTVWQSpAetU+fAFZJ05CJIPVJEFJQnbSNtuCrCJoN0dvV\n2oFZy4ycNdMWr8fWXW04sGsBzu3XA0MG9MKFJ/bE4UVZImsyYSXoTkTZ6bbd+m1Qycc+CXmBROmo\n4jBKZ8vOVn593ioe9vzcfUr+/eal+TxpwVpu2dPurH0qLhu3JoucuIvy0d+dj30S8gKIDz+BC2LY\ntHUXPz9nJVeM6Sz5d8aoKXzr+AX8ztJm65J/dvDTb+x0235FLUi6BCFiiOAza7XIPm/ewU/NXM7/\n88TsvZOuA++fwXdPXMwf5VLyT5Vc+qBLlMJo0YaxzYLgEBF8ZkcWarLk3wO1n/HFD83cK/LfffQd\nfmzaUl7qZsm/dFLDC5OZJLO5n3QXDFcNwwyC5SuRNEIEURX8/J60tRlV0RFnzF3ZGVmzZssuFBBw\nRu8jMbisFwaV9cIxXzlIX/vSyRZNojo568fEopsTx3aRSBohgkiUDqAkfrvbOvDu8g2orW/C1MXr\nsWlnK7p1KcC5/bpjUFkvXHRSTxx5cDd9bTJDV/ilH4IXpOiVILVFEDxCBB8wFdHtT43G9K+fj8n1\n6zBziVHy79BEyb/BZb1wfv8eOPgAj4uBWQmVqpD7IXhBsqqDdLchCB4hJQ6BfUreNW/Yhilnfhu1\nAy/He0u6oW3Rx+h+yAG45JRjMLisJ87p2x3duhTo3b6dB36sSgmqlpAzyw6puYTifm0ISnk7u2UO\nBSFC5LXgr9rUgtqSs1E74gXUNWwGM1DSrQjXnNoTg8t64dSSI1BYQO5sPN3SzFaXtbrasJKTNVxT\nSYqmqpD7IXh+DDLZqKgQgReETKjM7Hr1chqlE4/HedGXW/nhKUt4SErJvyF/m8UPT1nCi77cqh5Z\n4zTqRDVaJFuxj0yZO4Ma166r8EkQIn0EIWQgSmGZy9dv5ztr6vlb93aW/LvsqXd5zKwV3LBhp/0V\n6ghtVH1K1WxgKCwMhuB5Fdeus36BIESMSAn+lPqmfUr+rd+2O/OCOlIOq6K6Dp9Lo1niVVy7avlE\neahKEPZDVfDzIkqntT2O3e0d2Uv+2Yne0BF1EuS4eTt4FYGjsp2g7ytB8AnVKB3NYSkZGzKEiJYQ\n0XIiGuHGNrp1Kcgu9oAxiZkqvoDxubJy/2XNokvsRJ1UVBjiHosZohWLZR5cRo0yBoJU/JzwTEfH\nvtC1HatIpjBSXW0MZAUFxt/qar9bJOQzKrcBub4AFAJYAaAPgG4APgFwstnyrmbLtOM68bpaU3Fx\n53aKi4PlogiSDz/f0iaIi0rQBILgwwdwNoDalM8jAYw0W95VwbcrFl5MDoblgvdqolRHDeIwkW8D\nmOAbqoLvqg+fiC4DMISZr0t8vgrAmcx8Y8oywwAMA4CSkpLTGzL5aHUQxCcwxSdtH13Vq4JAkJ5Q\nFkJNYHz4VjDzaGYuZ+byHj16uLchVZ+6l+SjT9ptKiqMwTAeN/6GVewB7+ZHBCGB24K/BsBxKZ+P\nTXznD0ETC7ngo03QJ+yFvMNtwZ8LoB8R9SaibgCuADDB5W26i86oCrngo00Q7zqFvMbVXDrM3E5E\nNwKohRGx8xwz17u5TVexkx9HBUn0JUjeH8FD8uLBK8/QOcmaT5OPgiD4iqRHdgNdk6y67xQEQRAU\n8D1KRxtePLGoa5LVzlO/giAImsgPwU9azA0NRlxz0mLWLfpOJllTBySzZw0kHFMQBBfJD8H3ymLO\nNaoifUAyQ+VOQXKvCIKQI/nhw/fyASaVqIr0CdkdO/YfkNJRuVMQ378gCA7IDwvfiweYVC3rTO6l\njRvN12vnTkF8/4IgOCA/BD+Tb53IEFsdbg87cwSZRNmMWMzeU78qdzLi8hEEwYT8EPxU3zpgiH3S\nV+5kAjcpnldeqW5ZqyZ/y+WJWqs7Ga8mrwVBCCX5IfhAZ56cWGz/idFc3B6p4mlGumXdvbv5ssXF\nzh+ht4oSEpdPNJC7OCFXVHIoe/XSkg9fV41Ys1zlmfKWZ8rTnr5tXTnbs+WMD3p9XME5+VYTQNAC\nFPPh54+Fn0TXBK5VhI+VZZ0Ks74omuSdzLhxxuerruq08iT7Zv4jd3GCA/JP8HVloMwmkkmXDGCe\nXyd9eZ2Y+eqHDpXsm/mO1FAQHJB/gq8r5azZwFFV1ZkozcrHn/yNbsE1s/ImTpR0u/mO3MUJDpBs\nmdnIltFSxbIvLgYeeUS/4EppvOgSxFKdgu+EpsShVnRHL2SrkJXtFjoWM+4ENmxw5yIUKy+6SNEU\nwQH5kVoB8D7tQEmJfwXIR43KbOWJrz4aSNEUIUfCb+Hn8nCUDvwsTyhWniAIORBuCz+TPzMdt6IX\n/C5PKFaeIAg2CfekrWpIpNsuFkEQBB+JxqStnYejBEEQIk64BV/l4ShxewiCIAAIu+BbPRwlYi8I\ngrCXcAu+RKsIgiAoE+4oHUCiVQRBEBQJt4UvCIIgKCOCLwiCEBFE8AVBECKCCL4gCEJEEMEXBEGI\nCCL4giAIEUEEXxAEISKI4AuCIEQEEXxBEISIIIIvCIIQEUTwBUEQIoIjwSei24loDRF9nHgNTfnf\nSCJaTkRLiGiw86a6iO7i54IgCAFER/K0h5n5gdQviOhkAFcAKAPwVQBTiegEZu7QsD29eF38XBAE\nwSfcculcCuAlZt7DzF8AWA7gDJe25YzKSm+LnwuCIPiEDsG/kYg+JaLniOiIxHfHAFiVsszqxHfB\nw6xMolvFzwVBEHzCUvCJaCoRLczwuhTAUwD6AjgFwFoAD9ptABENI6I6Iqprbm623QHHmJVJzFY+\nURAEIYRYCj4zX8TMAzK83mDmdczcwcxxAGPQ6bZZA+C4lNUcm/gu0/pHM3M5M5f36NHDaX/MMZuY\nNSuTKMXPBUHIMxxN2hLR0cy8NvHx+wAWJt5PAPACET0EY9K2H4APnWzLESoTs5WVhhunpMQQe5mw\nFQQhzyBmzv3HRONguHMYwEoAv0gOAERUCeDnANoB3MzMk6zWV15eznV1dTm3x5TSUkPk04nFjGLn\ngiAIIYaI5jFzueVyTgRfN64JfkEBkKmfREA8rn97giAIHqIq+NF40lYmZgVBECIi+DIxKwiCEBHB\nr6gARo82fPZExt/Ro2ViVhCESKEjtUI4qKgQgRcEIdJEw8IXBEEQRPAFQRCiggi+IAhCRBDBFwRB\niAgi+IIgCBEhUE/aElEzgAw5EJToDmCDxuaEhSj2W/ocDaTP6sSY2TL7ZKAE3wlEVKfyaHG+EcV+\nS5+jgfRZP+LSEQRBiAgi+IIgCBEhnwR/tN8N8Iko9lv6HA2kz5rJGx++IAiCkJ18svAFQRCELIRO\n8IloCBEtIaLlRDQiw/8PIKKXE///gIhKvW+lXhT6/FsiWkREnxLRNCKK+dFO3Vj1O2W5HxARE1Ho\nIzpU+kxEP0wc73oiesHrNupG4fwuIaIZRDQ/cY4P9aOduiCi54hoPREtNPk/EdGjif3xKRGdpm3j\nzByaF4BCACsA9AHQDcAnAE5OW+ZXAJ5OvL8CwMt+t9uDPg8EUJR4f0PY+6za78RyhwKYBeB9AOV+\nt9uDY90PwHwARyQ+H+V3uz3o82gANyTenwxgpd/tdtjncwGcBmChyf+HApgEgACcBeADXdsOm4V/\nBoDlzPw5M7cCeAnApWnLXApgbOL9awAuJCLysI26sewzM89g5kSFdrwP4FiP2+gGKscaAP4K4F4A\nu71snEuo9Pl6AE8w82YAYOb1HrdRNyp9ZgCHJd4fDuBLD9unHWaeBWBTlkUuBfA8G7wP4CtEdLSO\nbYdN8I8BsCrl8+rEdxmXYeZ2AFsBFHvSOndQ6XMq18KwDsKOZb8Tt7rHMfObXjbMRVSO9QkATiCi\nd4nofSIa4lnr3EGlz7cDuJKIVgOYCOAmb5rmG3aveWWiUwAlAhDRlQDKAZznd1vchogKADwE4Bqf\nm+I1XWC4dc6HcSc3i4i+xsxbfG2Vu/wYwD+Z+UEiOhvAOCIawMxxvxsWNsJm4a8BcFzK52MT32Vc\nhoi6wLgF3OhJ69xBpc8goosAVAK4hJn3eNQ2N7Hq96EABgCYSUQrYfg6J4R84lblWK8GMIGZ25j5\nCwBLYQwAYUWlz9cCeAUAmHkOgANh5JzJV5Su+VwIm+DPBdCPiHoTUTcYk7IT0paZAODqxPvLAEzn\nxExISLHsMxGdCuDvMMQ+7D7dJFn7zcxbmbk7M5cycymMuYtLmLnOn+ZqQeX8/jcM6x5E1B2Gi+dz\nLxupGZU+NwK4EACI6CQYgt/saSu9ZQKAnyaidc4CsJWZ1+pYcahcOszcTkQ3AqiFMbv/HDPXE9Ed\nAOqYeQKAZ2Hc8i2HMTFyhX8tdo5in+8HcAiAVxPz043MfIlvjdaAYr/zCsU+1wIYRESLAHQA+AMz\nh/YOVrHPvwMwhoh+A2MC95owG3FE9CKMQbt7Yl7iNgBdAYCZn4YxTzEUwHIALQB+pm3bId5vgiAI\ngg3C5tIRBEEQckQEXxAEISKI4AuCIEQEEXxBEISIIIIvCIIQEUTwBUEQIoIIviAIQkQQwRcEQYgI\n/x8ccE760Uce6wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZfyGoCQSsb_",
        "colab_type": "text"
      },
      "source": [
        "# Dataset + DataLoader + Compose를 하면 병렬화가되는 하나의 파이프라인이 완성 #\n",
        "1. Dataset 구현\n",
        "2. DataLoader 활용\n",
        "3. Transform구현과 Compose 활용\n",
        "\n",
        "이 모든 과정은 하나의 파이프라인으로 만들어서 일괄적으로 처리하여 훈련셋 학습 과정을 효과적으로 하자 입니다. 이것을 한 이유는 실제 현장에서 처리하다보면 데이터 사이즈가 방대합니다. 이럴 경우 병렬처리 분산처리등을 동원하여 학습을 해야 합니다. 그럴려면 약간의 규약을 지켜주면 복잡한 구현없이 개발자는 이 유용한 기능들을 사용할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0vWcXyl1RUv",
        "colab_type": "text"
      },
      "source": [
        "# PyTorch의 DataSet Loader 구현 #\n",
        "자 그럼 조금 더 PyTorch framework에 알맞게 구조화를 시켜 보겠습니다. PyTorch에서는 pytorch.utils.data.Dataset라는 클래스를 제공합니다. 이것은 추상클래스입니다. 역할 명세서 정의는 되어있지만, 실제적으로 작동부분에 해당하는 구현이 없는 껍데기 클래스입니다. 저의가 할 일을은 Dataset의 부족한 부분을 구현해주면 됩니다. 보통 구현할 내용은 다음과 같습니다:\n",
        "1. \\__init__(): Dataset이 만들어질때 필요한 부분입니다. 파일로부터 메모리에 적재하는 과정을 구현합니다.\n",
        "2. \\__len__(): 데이터셋의 크기를 반환합니다. 전체 루프를 돌때 필요한 부분입니다.\n",
        "3. \\__getitem__(): 데이터셋의 접근할 수 있는 방법입니다. 인자로 idx를 받으며, 배열 접근처럼 가능해집니다. \n",
        "4. DataLoader 클래스와 같이 활용한다.\n",
        "\n",
        "Dataset을 구현할때 일부분만 메모리에 적재하도록 짜는 역할은 개발자의 몫입니다. 안타깝지만 자동은 없습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9B3dnDD42h-",
        "colab_type": "text"
      },
      "source": [
        "보통은 파일로 저장하고 읽기를 하니다. 그래서 2가지로 코드 구현을 해보겠습니다.\n",
        "# x,y데이터를 csv형식의 파일로 저장 #"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QA0NZPld33z9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "948992e3-9fa4-4d79-9d1c-39227c093f79"
      },
      "source": [
        "# 실험을 위해서 x, y를 csv형식의 파일로 저장을 하겠습니다.\n",
        "with open(\"x.csv\", \"w\") as f:\n",
        "  for v in x.numpy():\n",
        "    f.write(\"{}\\n\".format(v))\n",
        "\n",
        "with open(\"y.csv\", \"w\") as f:\n",
        "  for v in y.numpy():\n",
        "    f.write(\"{}\\n\".format(v))\n",
        "\n",
        "print(\"x.csv의 상위 10개\")\n",
        "!head -n 10 x.csv\n",
        "print(\"y.csv의 상위 10개\")\n",
        "!head -n 10 y.csv"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x.csv의 상위 10개\n",
            "0.0\n",
            "9.765720278664958e-06\n",
            "1.9531440557329915e-05\n",
            "2.929715992650017e-05\n",
            "3.906288111465983e-05\n",
            "4.882860230281949e-05\n",
            "5.859431985300034e-05\n",
            "6.836004467913881e-05\n",
            "7.812576222931966e-05\n",
            "8.789147977950051e-05\n",
            "y.csv의 상위 10개\n",
            "-28.275196075439453\n",
            "-29.069730758666992\n",
            "-2.0152087211608887\n",
            "-7.513141632080078\n",
            "30.965608596801758\n",
            "26.26564598083496\n",
            "-3.973937511444092\n",
            "-57.94906234741211\n",
            "15.176841735839844\n",
            "-32.390377044677734\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9h2UIk1N5Fro",
        "colab_type": "text"
      },
      "source": [
        "# csv 파일을 Dataset으로 적제 #\n",
        "보통은 파일로부터 다 읽어 들이지 않고, 일부 데이터를 캐싱하면서 처리를 합니다. Dataset의 작동원리를 익히는 것이므로 복잡하지 않게 그냥 다 메모리에 적제해 보도록 하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXNl2lyx6L3C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.utils.data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzBan7uW5nbd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, transform=None):\n",
        "    super(MyDataset, self).__init__()\n",
        "    self.x = self.load(\"x.csv\")\n",
        "    self.y = self.load(\"y.csv\")\n",
        "    self.transform = transform\n",
        "  \n",
        "  def load(self, filename):\n",
        "    data = []\n",
        "    with open(filename, \"r\") as f:\n",
        "      for line in f.readlines():\n",
        "        line = line.strip()  # 공백제거\n",
        "        if line == \"\":\n",
        "          continue\n",
        "        data.append( float(line) )\n",
        "      return torch.tensor(data)\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.x)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if self.transform:  # 변환이 있다면 x를 적용후 반환한다.\n",
        "      return {'x': self.transform(self.x[idx]), 'y': self.y[idx]}\n",
        "    return {'x': self.x[idx], 'y': self.y[idx]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "We6si6WT6HYz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f617ac45-069f-4601-d806-0dd52691ad8d"
      },
      "source": [
        "# MyDataset 생성\n",
        "dataset = MyDataset()\n",
        "print(\"len(dataset)\", len(dataset))"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len(dataset) 102400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOac_3PU7E6g",
        "colab_type": "text"
      },
      "source": [
        "## 배열처럼 접근 ##\n",
        "\\__getitem__을 구현하고 반환을 x와 y를 짝을 이루게 반환을 시켰습니다. 파이썬은 여러개를 반환할 수 있는 언어입니다.\n",
        "1. 하나로 접근도 해본다.\n",
        "2. 여러개로도 접근해 본다. 이걸 slice방식이라고 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmLmWXyU6dGr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "7864d38e-f329-4f67-d92b-2ef2058f9d3d"
      },
      "source": [
        "print(\"배열처럼 접근\", dataset[0])\n",
        "print(\"Slice처럼 배열처럼 접근\", dataset[0:32])"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "배열처럼 접근 {'x': tensor(0.), 'y': tensor(-28.2752)}\n",
            "Slice처럼 배열처럼 접근 {'x': tensor([0.0000e+00, 9.7657e-06, 1.9531e-05, 2.9297e-05, 3.9063e-05, 4.8829e-05,\n",
            "        5.8594e-05, 6.8360e-05, 7.8126e-05, 8.7891e-05, 9.7657e-05, 1.0742e-04,\n",
            "        1.1719e-04, 1.2695e-04, 1.3672e-04, 1.4649e-04, 1.5625e-04, 1.6602e-04,\n",
            "        1.7578e-04, 1.8555e-04, 1.9531e-04, 2.0508e-04, 2.1485e-04, 2.2461e-04,\n",
            "        2.3438e-04, 2.4414e-04, 2.5391e-04, 2.6367e-04, 2.7344e-04, 2.8321e-04,\n",
            "        2.9297e-04, 3.0274e-04]), 'y': tensor([-28.2752, -29.0697,  -2.0152,  -7.5131,  30.9656,  26.2656,  -3.9739,\n",
            "        -57.9491,  15.1768, -32.3904,  16.0102,  14.7558,   9.1081,  42.6437,\n",
            "         39.0184,  -1.9022, -35.0624, -45.3597,  22.5189,  29.3257,  23.4867,\n",
            "        -41.1303,  -4.7172,  61.1149,  28.0315, -12.0381,   0.3260,  11.0333,\n",
            "         47.2111,  53.1212,  33.9212, -19.7770])}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttQBw-Vi7lPf",
        "colab_type": "text"
      },
      "source": [
        "##  MyDataset, DataLoader를 활용해서 SGD에 붙여보자 ##\n",
        "\n",
        "크게 바뀐 코드는 없습니다. 기존에 x,y를 쓰던것을 Dataset으로 교체되었습니다. 이것으로 실제 구현 파트가 잘 캡슐화되어 활용하는 입장에서는 고민하지 않고 쓰게 됩니다. 내부적으로 훈련셋을 증폭하기 위해서 잡음을 섞거나, 순서를 무작위로 변경하고나, 캐시하는 등 복잡한 과정이 숨겨지는 장점이 있습니다.\n",
        "\n",
        "### DataLoader ###\n",
        "DataLoader는 Dataset과 mini batch의 크기를 받아서, 훈련할 수 있는 mini batch크기에 훈련셋을 반환해 주는 반복자입니다. 또한 가끔은 처리보다 파일에서 메모리로 적제하는게 느린 경우가 있습니다. 학습 처리 속도에 지장을 최소화 하도록 병렬로 처리를 합니다. 물론 자원이 충분해야 합니다. 예를들어 CPU가 4개 달린 머신이어야 합니다. 이 부분도 저희가 전체를 구현없이 가능하도록 DataLoader가 지원합니다.\n",
        "\n",
        "\n",
        "1. Dataset: 데이터를 파일로부터 읽는 부분\n",
        "2. batch_size: Mini Batch의 크기\n",
        "3. shuffle: 훈련셋을 섞는 기능, 매번 같은 훈련셋이 아니도록 만들어 주는 기능. 훈련셋들간에 독립적이 되도록 하는 목적\n",
        "3. num_workers가 병렬처리의 개수를 지정합니다. 이것은 하나하나씩 Dataset.\\_\\_getitem\\_\\_을 호출합니다. 그리고 내부적으로 mini batch크기로 취합처리해서 하나로 반환해 줍니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BOFYhlA6zco",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "c902a04d-4631-4334-ad8f-0d14d740d443"
      },
      "source": [
        "# Model 정의\n",
        "model = Model()\n",
        "\n",
        "# Optimizer 설정\n",
        "epoch = 10\n",
        "mini_batch = 1024\n",
        "learning_rate = 1e-4\n",
        "optimizer = torch.optim.SGD(params=model.parameters(), lr=learning_rate)\n",
        "\n",
        "# MyDataset을 구현\n",
        "dataset = MyDataset()\n",
        "L_list = []\n",
        "\n",
        "\n",
        "nsample = len(dataset)\n",
        "print(\"훈련셋 개수\", nsample)\n",
        "L_sum = 0\n",
        "\n",
        "loader = torch.utils.data.DataLoader(dataset, batch_size=mini_batch, shuffle=True, num_workers=4)\n",
        "for e_iter in range(epoch):  # epoch loop\n",
        "  for step, mini_batch in enumerate(loader):   # minibatch loop\n",
        "    pred_y = model(mini_batch['x'])\n",
        "    L = loss(pred_y, mini_batch['y'])\n",
        "    L_sum += L\n",
        "    optimizer.zero_grad() \n",
        "    L.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 50 == 0:\n",
        "      print(\"Epoch:{}/Step:{}, L:{:.5}, w={}, b={:.3}, grad(w)={:.3}, grad(b)={:.3}\".format(e_iter, step, L.item(), model.w.item(), model.b.item(), model.w.grad.item(), model.b.grad.item()))\n",
        "    step += 1\n",
        "  print(\"==> End Epoch #{}, loss=L:{:.5}\".format(e_iter, L_sum/max_step))"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "훈련셋 개수 102400\n",
            "Epoch:0/Step:0, L:5.4652e+06, w=8.727423667907715, b=12.6, grad(w)=-7.96e+04, grad(b)=-1.19e+05\n",
            "Epoch:0/Step:50, L:1.0365e+06, w=68.2392578125, b=27.7, grad(w)=-4.31e+03, grad(b)=5.31e+03\n",
            "==> End Epoch #0, loss=L:1.2224e+06\n",
            "Epoch:1/Step:0, L:9.4121e+05, w=89.37413024902344, b=16.8, grad(w)=-4.18e+03, grad(b)=1.56e+02\n",
            "Epoch:1/Step:50, L:9.2445e+05, w=99.7844009399414, b=11.1, grad(w)=-1.44e+03, grad(b)=9.9e+02\n",
            "==> End Epoch #1, loss=L:2.1603e+06\n",
            "Epoch:2/Step:0, L:9.1439e+05, w=104.68040466308594, b=8.01, grad(w)=1.23e+03, grad(b)=2.42e+03\n",
            "Epoch:2/Step:50, L:8.6979e+05, w=107.48329162597656, b=6.7, grad(w)=1.46e+03, grad(b)=2.96e+03\n",
            "==> End Epoch #2, loss=L:3.0849e+06\n",
            "Epoch:3/Step:0, L:9.2409e+05, w=108.84705352783203, b=5.69, grad(w)=5.22e+02, grad(b)=1.42e+03\n",
            "Epoch:3/Step:50, L:9.216e+05, w=109.58184051513672, b=5.59, grad(w)=-1.57e+03, grad(b)=-1.62e+03\n",
            "==> End Epoch #3, loss=L:4.0087e+06\n",
            "Epoch:4/Step:0, L:9.0182e+05, w=110.17737579345703, b=5.38, grad(w)=1.16e+03, grad(b)=3.04e+03\n",
            "Epoch:4/Step:50, L:9.5831e+05, w=110.45574951171875, b=5.57, grad(w)=-1.34e+03, grad(b)=-2.37e+03\n",
            "==> End Epoch #4, loss=L:4.9324e+06\n",
            "Epoch:5/Step:0, L:8.9467e+05, w=110.39944458007812, b=5.29, grad(w)=6.86e+02, grad(b)=7.31e+02\n",
            "Epoch:5/Step:50, L:1.0019e+06, w=110.38617706298828, b=5.23, grad(w)=-2.52e+03, grad(b)=-3.16e+03\n",
            "==> End Epoch #5, loss=L:5.8561e+06\n",
            "Epoch:6/Step:0, L:9.5455e+05, w=110.52814483642578, b=5.54, grad(w)=1.15e+03, grad(b)=1.51e+02\n",
            "Epoch:6/Step:50, L:9.1112e+05, w=110.38717651367188, b=5.72, grad(w)=-1.44e+03, grad(b)=-2.83e+03\n",
            "==> End Epoch #6, loss=L:6.7798e+06\n",
            "Epoch:7/Step:0, L:9.2431e+05, w=110.41114807128906, b=5.04, grad(w)=2.34e+02, grad(b)=2.26e+02\n",
            "Epoch:7/Step:50, L:1.003e+06, w=110.47522735595703, b=5.18, grad(w)=6.61e+02, grad(b)=-2.42e+02\n",
            "==> End Epoch #7, loss=L:7.7035e+06\n",
            "Epoch:8/Step:0, L:9.2647e+05, w=110.35822296142578, b=4.98, grad(w)=-8.55e+02, grad(b)=-1.06e+03\n",
            "Epoch:8/Step:50, L:9.2308e+05, w=110.60589599609375, b=5.13, grad(w)=-9.94e+02, grad(b)=-1.49e+03\n",
            "==> End Epoch #8, loss=L:8.6272e+06\n",
            "Epoch:9/Step:0, L:9.1678e+05, w=110.20613861083984, b=5.15, grad(w)=9.12e+02, grad(b)=1.74e+03\n",
            "Epoch:9/Step:50, L:9.0417e+05, w=110.17778015136719, b=4.91, grad(w)=5.15e+02, grad(b)=3.96e+02\n",
            "==> End Epoch #9, loss=L:9.5508e+06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tMkG9WI9N9i",
        "colab_type": "text"
      },
      "source": [
        "# Transform를 붙여보자 #\n",
        "PyTorch는 복잡한 변환과정을 쉽게하여 가독성을 높이도록 지원합니다. 물론 사람마다 다르므로, 반듯이 지키실 필요는 없습니다.\n",
        "\n",
        "MyDataset을 구현할 때 인자로 받을 계획입니다. 이러면 MyDataset외부에 정의된 변환이 적용된 훈련셋에 적용되어서 받을 것입니다. 특히 이것은 영상처리에서 유용하게 쓰입니다. 보통은 Dataset은 미리 정의되어 있고, 변환 방식은 나중에 정의되기 때문입니다. 그리고 이 변환과정이 좀 연산이 많습니다. 병렬처리를 하지 않으면 모델 훈련 속도 저하가 발생합니다. 그래서 병렬처리로 변환을 독립적으로 진행합니다. 이것을 도와주는 것이 DataLoader였습니다.\n",
        "\n",
        "**Transform은 함수처럼 호출 가능한 클래스**로 구현되어야 합니다. 단 병렬처리가 용이하도록 설계를 해야 합니다. 그래서 **\\_\\_call_\\_**을 구현해야 합니다. 관점은 그냥 함수 구현하듯 구현하시면 됩니다.\n",
        "\n",
        "변환을 여러단계로 해야 할 경우 모듈화가 되어 상당히 용히 합니다. 변환 과정을 공장처리 과정처럼 체인처럼 묶는 것입니다. 이것을 PyTorch에서는  **transforms.Compose**로 지원을 합니다.\n",
        "\n",
        "여기서 구현할 것은 랜덤으로 숫자를 추가해주는 방식입니다. 훈련셋에 잡음을 추가해주는 방식이라 할 수 있습니다. 많이 쓰는 방식입니다. 무조건 좋은 것은 아니고 상황에따라서 판단해야 합니다. 타당성보다는  이해를 목적으로 구현을 해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgpgaIKW76LI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AddNoise(object):\n",
        "  def __init__(self, noise_level):\n",
        "    self.noise_level = noise_level\n",
        "  \n",
        "  def __call__(self, x):\n",
        "    return x + torch.randn(x.size())*self.noise_level\n",
        "\n",
        "class ReScale(object):\n",
        "  def __init__(self, scale):\n",
        "    self.scale = scale\n",
        "  \n",
        "  def __call__(self, x):\n",
        "    return x*self.scale"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpUBfEit_mfg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "9ce161f1-1e83-4052-9208-b8830a1f8583"
      },
      "source": [
        "add_noise = AddNoise(0.0001)\n",
        "rescale = ReScale(-1.0)   # 음수값 처리\n",
        "\n",
        "test_input =  torch.tensor([0.1,0.2], dtype=torch.float32)\n",
        "print(\"test_input\", test_input)\n",
        "print(\"add_noise1 변환\", add_noise(test_input))\n",
        "print(\"rescale 변한\", rescale(test_input))\n",
        "print(\"복합 변환:\", rescale(add_noise(test_input)))"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_input tensor([0.1000, 0.2000])\n",
            "add_noise1 변환 tensor([0.0999, 0.2000])\n",
            "rescale 변한 tensor([-0.1000, -0.2000])\n",
            "복합 변환: tensor([-0.1000, -0.1999])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMiwY1qxBmdl",
        "colab_type": "text"
      },
      "source": [
        "## 복합 변환을 Compose로 묶어 버리자! ##\n",
        "\n",
        "보통은 이러한 변환이 영상처리에 많이 있습니다. 영상 처리는 많은 변환을 가지고 있어서, 사용자가 모든 것을 구현할 필요가 없습니다. 사실 지금 이 예에서는 이정도로 정성과 노력을 들일 필요는 없지만, 추후에 이해 목적으로 사용해 봅시다!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhVPyfHVAOXC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "33a67098-848e-41f3-bb37-aa20387b582c"
      },
      "source": [
        "import torchvision.transforms\n",
        "compose = torchvision.transforms.Compose([add_noise, rescale])\n",
        "print(compose(test_input))"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 0.9853, -0.8447])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qW2vgLYhCrCn",
        "colab_type": "text"
      },
      "source": [
        "## Transform과 Compose를 기존 SGD에 붙여 보자##\n",
        "지금은 매우 비효율적입니다. 이유는 Transform과정이나 캐싱이 불필요하기 때문입니다. 추후에 대용량으로 처리할 때 도움이 될 것입니다. 그때 기억해 내시면 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmPPYDP2Bzm0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "2870e2cb-c293-408c-f50a-2ebea51777ea"
      },
      "source": [
        "# Model 정의\n",
        "model = Model()\n",
        "\n",
        "# Optimizer 설정\n",
        "epoch = 10\n",
        "mini_batch = 1024\n",
        "learning_rate = 1e-4\n",
        "optimizer = torch.optim.SGD(params=model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Compose 정의\n",
        "add_noise = AddNoise(1e-5)\n",
        "rescale = ReScale(-1.0)\n",
        "compose = torchvision.transforms.Compose([add_noise, rescale])\n",
        "\n",
        "\n",
        "# MyDataset에 compose를 연결한다.\n",
        "dataset = MyDataset(transform=compose)\n",
        "L_list = []\n",
        "\n",
        "nsample = len(dataset)\n",
        "loader = torch.utils.data.DataLoader(dataset, batch_size=mini_batch, shuffle=True)\n",
        "print(\"훈련셋 개수\", nsample)\n",
        "L_sum = 0\n",
        "for e_iter in range(epoch):  # epoch loop\n",
        "  for step, mini_batch in enumerate(loader):   # minibatch loop\n",
        "    pred_y = model(mini_batch['x'])\n",
        "    L = loss(pred_y, mini_batch['y'])\n",
        "    L_sum += L\n",
        "    optimizer.zero_grad() \n",
        "    L.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 50 == 0:\n",
        "      print(\"Epoch:{}/Step:{}, L:{:.5}, w={}, b={:.3}, grad(w)={:.3}, grad(b)={:.3}\".format(e_iter, step, L.item(), model.w.item(), model.b.item(), model.w.grad.item(), model.b.grad.item()))\n",
        "    step += 1\n",
        "  print(\"==> End Epoch #{}, loss=L:{:.5}\".format(e_iter, L_sum/max_step))"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "훈련셋 개수 102400\n",
            "Epoch:0/Step:0, L:5.7719e+06, w=-7.136871814727783, b=12.3, grad(w)=8.12e+04, grad(b)=-1.23e+05\n",
            "Epoch:0/Step:50, L:1.1374e+06, w=-67.78055572509766, b=27.4, grad(w)=6.07e+03, grad(b)=4.48e+03\n",
            "==> End Epoch #0, loss=L:1.2326e+06\n",
            "Epoch:1/Step:0, L:9.2984e+05, w=-88.94990539550781, b=16.9, grad(w)=2.71e+03, grad(b)=2.26e+03\n",
            "Epoch:1/Step:50, L:9.2673e+05, w=-99.90492248535156, b=11.3, grad(w)=3.59e+03, grad(b)=-2.47e+03\n",
            "==> End Epoch #1, loss=L:2.1709e+06\n",
            "Epoch:2/Step:0, L:9.3775e+05, w=-104.65281677246094, b=8.22, grad(w)=1.14e+03, grad(b)=-1.07e+03\n",
            "Epoch:2/Step:50, L:9.0497e+05, w=-107.83969116210938, b=6.47, grad(w)=9.35e+02, grad(b)=-9.86e+02\n",
            "==> End Epoch #2, loss=L:3.0957e+06\n",
            "Epoch:3/Step:0, L:9.3068e+05, w=-108.66633605957031, b=6.24, grad(w)=4.52e+02, grad(b)=-1.74e+03\n",
            "Epoch:3/Step:50, L:8.7983e+05, w=-109.3499526977539, b=5.52, grad(w)=1.41e+03, grad(b)=-2.22e+03\n",
            "==> End Epoch #3, loss=L:4.0195e+06\n",
            "Epoch:4/Step:0, L:9.4374e+05, w=-109.9946517944336, b=5.51, grad(w)=-91.7, grad(b)=-1.43e+03\n",
            "Epoch:4/Step:50, L:8.9438e+05, w=-110.01034545898438, b=5.13, grad(w)=3.29e+02, grad(b)=-9.77e+02\n",
            "==> End Epoch #4, loss=L:4.9432e+06\n",
            "Epoch:5/Step:0, L:9.1188e+05, w=-110.36754608154297, b=5.39, grad(w)=-19.7, grad(b)=-1.27e+03\n",
            "Epoch:5/Step:50, L:9.398e+05, w=-110.54531860351562, b=5.11, grad(w)=-6.88e+02, grad(b)=8.79e+02\n",
            "==> End Epoch #5, loss=L:5.8668e+06\n",
            "Epoch:6/Step:0, L:1.0038e+06, w=-110.46481323242188, b=5.42, grad(w)=1.04e+03, grad(b)=-2.5e+02\n",
            "Epoch:6/Step:50, L:9.4647e+05, w=-110.22555541992188, b=4.64, grad(w)=-4.88e+02, grad(b)=7.47e+02\n",
            "==> End Epoch #6, loss=L:6.7905e+06\n",
            "Epoch:7/Step:0, L:9.1402e+05, w=-110.4823226928711, b=5.42, grad(w)=-9.58e+02, grad(b)=2.44e+03\n",
            "Epoch:7/Step:50, L:9.299e+05, w=-110.55292510986328, b=5.4, grad(w)=-1.62e+03, grad(b)=1.69e+03\n",
            "==> End Epoch #7, loss=L:7.7142e+06\n",
            "Epoch:8/Step:0, L:8.9252e+05, w=-110.21011352539062, b=4.87, grad(w)=-2.66e+02, grad(b)=1.13e+03\n",
            "Epoch:8/Step:50, L:8.8965e+05, w=-110.02949523925781, b=5.32, grad(w)=-5.34e+02, grad(b)=-99.5\n",
            "==> End Epoch #8, loss=L:8.638e+06\n",
            "Epoch:9/Step:0, L:9.0054e+05, w=-110.60665130615234, b=5.12, grad(w)=-1.62e+03, grad(b)=3.22e+03\n",
            "Epoch:9/Step:50, L:1.0225e+06, w=-110.6971664428711, b=5.25, grad(w)=1.08e+03, grad(b)=-3.05e+03\n",
            "==> End Epoch #9, loss=L:9.5616e+06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdild230DAwl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}