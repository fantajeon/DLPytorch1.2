{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fantajeon/DLPytorch1.2/blob/master/Chapter1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1amU_dj1bdOg",
        "colab_type": "text"
      },
      "source": [
        "# PyTorch 1.2 버전 설치"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32zRrpCKlhwZ",
        "colab_type": "text"
      },
      "source": [
        "Author: Hyeokjune Jeon(fantajeon@gmail.com)\n",
        "\n",
        "Colab을 통하여 작성했으며, 기본적인 패키지들은 설치가 되어있습니다. 예를들면, matplotlib같은 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDtTw0uqyGd_",
        "colab_type": "code",
        "outputId": "178e43ca-1885-4713-976e-9cfed62842ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!pip install --upgrade torch==1.2 torchvision==0.4"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: torch==1.2 in /usr/local/lib/python3.6/dist-packages (1.2.0)\n",
            "Requirement already up-to-date: torchvision==0.4 in /usr/local/lib/python3.6/dist-packages (0.4.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.2) (1.16.5)\n",
            "Requirement already satisfied, skipping upgrade: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4) (4.3.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision==0.4) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98gNrkT-bHoe",
        "colab_type": "text"
      },
      "source": [
        "PyTorch 버전은 1.2.0 기준입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0KzdZT4yOcN",
        "colab_type": "code",
        "outputId": "4bb0c717-c8de-4a36-8e95-c4da7c1ee680",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import torch\n",
        "torch.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.2.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPVxCeWJdmP6",
        "colab_type": "text"
      },
      "source": [
        "차후에 화면에 그래프를 그릴 것이기 때문에 미리 import 해 놓습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIeBQTcWZSeX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfeRT-1ubWLm",
        "colab_type": "text"
      },
      "source": [
        "# 이제 시작해 보겠습니다.\n",
        "이 번은 맛보기입니다. 한번 경험을 통해서 계속해서 개념들을 더해가면서, 딥러닝에 익숙해지도록 해보겠습니다. 많은 부분을 지금 당장 이해하지 않으셔도 됩니다. 바로 보고 이해하신다고 하면, 이미 기존에 기초가 탄탄하셨던 분일 것입니다. 너무 좌절하지 하지 마시고, 천천히 진행해 보겠습니다.\n",
        "\n",
        "## 꼭 알아야할 내용\n",
        "1. $x$,$y$로부터 $w$,$b$를 찾는다.\n",
        "2. 모델은 $y=wx + b$  이다.\n",
        "3. Loss 함수와 연관하여 $w$,$b$를 찾는 과정: gradient descent, learning rate.\n",
        "\n",
        "나열한 단어들간에 개념 연결입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lf8ra06Cyuj4",
        "colab_type": "text"
      },
      "source": [
        "다음 주어진 x, y 데이터를 근거로 w, b를 찾아라\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZAVys2Oyxv8",
        "colab_type": "text"
      },
      "source": [
        "$$y = wx + b$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7OfjV2ofh1U",
        "colab_type": "text"
      },
      "source": [
        "일반적으로 w, b의 값을 선언할때 값을 지정을 해줘야 합니다. 보통은 random하게 값을 지정해 줍니다. \n",
        "\n",
        "계속 진행하면서 전역변수 x, y, w, b임을 기억해주세요.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvCnX4IIy3TP",
        "colab_type": "code",
        "outputId": "4df93492-ff73-4b36-bc0f-69ec361fa87d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# dataset\n",
        "x = torch.tensor([0.1, 0.2, 0.3, 0.4, 0.5, 0.6], dtype=torch.float32)\n",
        "y = torch.tensor([15, 25, 40, 55, 65, 66], dtype=torch.float32)\n",
        "# requires_grad=True는 미분 값을 가진다는 것임.\n",
        "w = torch.rand(1, dtype=torch.float32, requires_grad=True)\n",
        "b = torch.rand(1, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "# model parameter, w and b\n",
        "def model(x):\n",
        "  return w*x + b\n",
        "\n",
        "pred_y = model(x)\n",
        "print(pred_y)"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.0548, 0.1048, 0.1547, 0.2047, 0.2546, 0.3046],\n",
            "       grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2QdPkxRI6R9",
        "colab_type": "text"
      },
      "source": [
        "이제 학습할 목표를 설계해야 합니다. x로부터 예측한 pred_y와 관촬된 y의 관계를 정의해야 합니다.\n",
        "여러가지 있지만 지금은 가장 단순한 방법인 두 변수의 오차의 제곱으로 해봅니다. \n",
        "$$L = \\frac{1}{2} \\sum { (\\text{pred_y} - y)^2 } $$를 합니다.\n",
        "보통 여기서 L을 loss 함수라고 합니다. 이 값을 줄이면, 우리가 원하는 w, b값을 정확히 찾을 수 있을 거라고 희망하면서 진행합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PcNi_yAKJAq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(pred_y, y):\n",
        "  return 0.5*(pred_y - y).pow(2.0).sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nL_5bt3Kc_-",
        "colab_type": "text"
      },
      "source": [
        "이제 이 것을 미분해서 기울기가 작은쪽으로 가도록 w, b를 천천히 움직여 봅시다. PyTorch에서는 backward() 함수를 호출하여, 이 두 변수의 미분값을 계산합니다. 신기합니다!! 자동으로 미분을 해줍니다. 이건 PyTorch가 연산 과정을 내부적으로 다 기록을 해놓고(computation graph칭함), 이 기록의 근거하여 자동으로 미분값을 계산할 수 있습니다. 이 방법을 gradient descent 방법이라 합니다. 가장 단순하면서 여기서부터 학습이 시작됩니다. 너무 많은 정보는 지금 어려울 수 있으니, 그렇다 치고 넘어갑시다.\n",
        "\n",
        "### Computation Graph\n",
        "그래프에 노드에 변수와 에지에 연산의 개념을 연결시켜서, 입력 변수들이 어떠한 복잡한 계산 과정거치면서 출력 변수까지 연결되어 있는지 기록되어 있는 자료구조입니다. 특히 이러한 과정을 입력에서 출력까지 그래프 계산하는 과정을 forward라고 하고, forward의 역순서로 오차와 미분값을가지고 그래프 연산을 전파하는 과정을 backward라고 합니다(backpropagation algorithm). 이러한 그래프 구조는 모든 정보를 포함하고 있어서 자동 미분이 가능해집니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KW_J4xq0KNET",
        "colab_type": "code",
        "outputId": "8fe9c5f8-5e95-4db4-f482-3224bcf82813",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "pred_y = model(x)\n",
        "L = loss(pred_y, y)\n",
        "L.backward()\n",
        "print(\"gradient of w\", w.grad)\n",
        "print(\"gradient of b\", b.grad)"
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gradient of w tensor([-112.1352])\n",
            "gradient of b tensor([-264.9218])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtbsXagHLIAg",
        "colab_type": "text"
      },
      "source": [
        "**Gradient Decendent 방법으로 최적화를 해보자!**\n",
        "$$\\begin{equation} \n",
        "w_{t+1} = w_{t} - \\eta \\frac{\\partial L}{\\partial w} \\\\\n",
        "b_{t+1} = b_{t} - \\eta \\frac{\\partial L}{\\partial b}\n",
        "\\end{equation}$$\n",
        "여기서 t와 t+1은 업데이트 순서입니다. 이것의 의미는 현재 값을 기준으로 미분한 방향에서 L값이 작은 쪽으로 값을 이동합니다. 그리고 $\\eta$는 learning rate라고 합니다. 이번 순서($t$)에서 $w$와  $b$를 얼마만큼 움직여서 오차($L$)를 줄여나갈지 결정해 줍니다. 주의할 점은 너무 크게 또는 작게 w와 b를 움직이면 학습에 실패할 수 있다.\n",
        "\n",
        "<img width='320px' src='https://fantajeon.github.io/DLPytorch1.2_Materials/images/gradient_descent.svg' />\n",
        "\n",
        "이 learning rate로 한번에 학습을 하지 않는다는 이야기는 반복적으로 적당하게 w,b를 움직이면서 L값을 관촬해야 한다는 것입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYHDiaDVZWse",
        "colab_type": "text"
      },
      "source": [
        "휴~ 많이 따라 오셨습니다. 이제 하나로 묶어서 L값을 관찰해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26vsCF4dKWg7",
        "colab_type": "code",
        "outputId": "bb5a55cb-8bd5-4bd0-b0ba-22b79c463828",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "learning_rate = 0.0001\n",
        "\n",
        "for step in range(300000):\n",
        "  # PyTorch는 Dynamic하게 연산을 기록하는 기능 때문에, \n",
        "  # 매번 loop에서 x-->loss-->backward()-->gradient descent를 해줘야 함.\n",
        "  pred_y = model(x)\n",
        "  L = loss(pred_y, y)\n",
        "  L.backward()\n",
        "\n",
        "  # torch.no_grad()는 computation graph에 기록하지 않는다.\n",
        "  with torch.no_grad():\n",
        "    w -= learning_rate * w.grad\n",
        "    b -= learning_rate * b.grad\n",
        "\n",
        "  if step % 10000 == 0:\n",
        "    print(\"Step:{}, L:{:.5}, w={:.3}, b={:.3}, grad(w)={:.3}, grad(b)={:.3}\".format(step, L.item(), w.item(), b.item(), w.grad.item(), b.grad.item()))\n",
        "\n",
        "  w.grad.zero_()\n",
        "  b.grad.zero_()"
      ],
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step:0, L:6970.6, w=0.522, b=0.0579, grad(w)=-2.24e+02, grad(b)=-5.3e+02\n",
            "Step:10000, L:644.71, w=28.8, b=35.0, grad(w)=-13.0, grad(b)=4.3\n",
            "Step:20000, L:484.52, w=40.7, b=30.8, grad(w)=-11.0, grad(b)=3.95\n",
            "Step:30000, L:367.14, w=50.9, b=27.1, grad(w)=-9.41, grad(b)=3.38\n",
            "Step:40000, L:281.12, w=59.6, b=24.0, grad(w)=-8.06, grad(b)=2.9\n",
            "Step:50000, L:218.09, w=67.0, b=21.3, grad(w)=-6.9, grad(b)=2.48\n",
            "Step:60000, L:171.91, w=73.4, b=19.0, grad(w)=-5.91, grad(b)=2.12\n",
            "Step:70000, L:138.06, w=78.9, b=17.0, grad(w)=-5.06, grad(b)=1.82\n",
            "Step:80000, L:113.26, w=83.6, b=15.3, grad(w)=-4.33, grad(b)=1.56\n",
            "Step:90000, L:95.085, w=87.6, b=13.9, grad(w)=-3.7, grad(b)=1.33\n",
            "Step:100000, L:81.768, w=91.0, b=12.7, grad(w)=-3.17, grad(b)=1.14\n",
            "Step:110000, L:72.009, w=94.0, b=11.6, grad(w)=-2.71, grad(b)=0.977\n",
            "Step:120000, L:64.857, w=96.5, b=10.7, grad(w)=-2.32, grad(b)=0.837\n",
            "Step:130000, L:59.62, w=98.6, b=9.93, grad(w)=-1.99, grad(b)=0.713\n",
            "Step:140000, L:55.779, w=1e+02, b=9.27, grad(w)=-1.7, grad(b)=0.612\n",
            "Step:150000, L:52.966, w=1.02e+02, b=8.7, grad(w)=-1.46, grad(b)=0.522\n",
            "Step:160000, L:50.902, w=1.03e+02, b=8.22, grad(w)=-1.25, grad(b)=0.449\n",
            "Step:170000, L:49.393, w=1.05e+02, b=7.81, grad(w)=-1.07, grad(b)=0.381\n",
            "Step:180000, L:48.286, w=1.06e+02, b=7.45, grad(w)=-0.917, grad(b)=0.326\n",
            "Step:190000, L:47.473, w=1.06e+02, b=7.15, grad(w)=-0.786, grad(b)=0.278\n",
            "Step:200000, L:46.878, w=1.07e+02, b=6.89, grad(w)=-0.672, grad(b)=0.241\n",
            "Step:210000, L:46.438, w=1.08e+02, b=6.66, grad(w)=-0.572, grad(b)=0.214\n",
            "Step:220000, L:46.12, w=1.08e+02, b=6.47, grad(w)=-0.493, grad(b)=0.177\n",
            "Step:230000, L:45.884, w=1.09e+02, b=6.31, grad(w)=-0.419, grad(b)=0.158\n",
            "Step:240000, L:45.715, w=1.09e+02, b=6.17, grad(w)=-0.361, grad(b)=0.136\n",
            "Step:250000, L:45.592, w=1.09e+02, b=6.05, grad(w)=-0.314, grad(b)=0.107\n",
            "Step:260000, L:45.495, w=1.1e+02, b=5.95, grad(w)=-0.267, grad(b)=0.0936\n",
            "Step:270000, L:45.431, w=1.1e+02, b=5.87, grad(w)=-0.232, grad(b)=0.079\n",
            "Step:280000, L:45.376, w=1.1e+02, b=5.79, grad(w)=-0.192, grad(b)=0.0787\n",
            "Step:290000, L:45.344, w=1.1e+02, b=5.73, grad(w)=-0.173, grad(b)=0.0548\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVtsudlDa8if",
        "colab_type": "text"
      },
      "source": [
        "w는 110근처, b는 5.7 근처에서 찾았습니다. 이제 화면에 그려보도록 하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_X1HvHRbEoG",
        "colab_type": "code",
        "outputId": "e5f5db05-306f-43fe-dadd-4837887e19b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "# numpy는 공학자들이 많이 쓰는 공학용 계산 framework입니다. PyTorch는 Numpy와 호환이 잘 되도록 설계되었습니다.\n",
        "x_numpy = x.numpy()\n",
        "y_numpy = y.numpy()\n",
        "# 아래에 detach는 연산을 기록하는 graph에서 때낸다. 향후 어떠한 연산을 해도 그래프에 기록되지 않을 것입니다.\n",
        "pred_y = model(x).detach().numpy()  \n",
        "plt.plot(x_numpy,y_numpy,'ro')\n",
        "plt.plot(x_numpy,pred_y,label='Fitted line')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f07eb175d68>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 193
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAWMcuq5FR+hjAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a63M9J0d6hF6",
        "colab_type": "text"
      },
      "source": [
        "다음에 더 자세한 이야기(Tensor, Numpy, Computation Graph, Backpropgation, Deep learning 등)를 진행해 보겠습니다. 오늘은 이만 이 정도까지 정리해보겠습니다.\n",
        "\n",
        "**감사합니다.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7nrWIAwGCiu",
        "colab_type": "text"
      },
      "source": [
        "#전체 코드입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyPUq26GGGaj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dataset\n",
        "x = torch.tensor([0.1, 0.2, 0.3, 0.4, 0.5, 0.6], dtype=torch.float32)\n",
        "y = torch.tensor([15, 25, 40, 55, 65, 66], dtype=torch.float32)\n",
        "w = torch.rand(1, dtype=torch.float32, requires_grad=True) \n",
        "b = torch.rand(1, dtype=torch.float32, requires_grad=True)  \n",
        "\n",
        "# model parameter, w and b\n",
        "def model(x):\n",
        "  return w*x + b\n",
        "\n",
        "def loss(pred_y, y):\n",
        "  return 0.5*(pred_y - y).pow(2.0).sum()\n",
        "\n",
        "learning_rate = 0.0001\n",
        "\n",
        "for step in range(300000):\n",
        "  pred_y = model(x)\n",
        "  L = loss(pred_y, y)\n",
        "  L.backward()\n",
        "\n",
        "  # torch.no_grad()는 computation graph에 기록하지 않는다.\n",
        "  with torch.no_grad():\n",
        "    w -= learning_rate * w.grad\n",
        "    b -= learning_rate * b.grad\n",
        "\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "  if step % 10000 == 0:\n",
        "    print(\"Step:{}, L:{:.5}, w={}, b={}\".format(step, L.item(), w.item(), b.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}